- Nach Kapitel über Klassifizierung und Labeling und Evaluationsframework soll noch ein Kapitel über das Frontend rein, wo die Sandbox, Labeling und Evaluierung gezeigt wird (Ich mache das glaub ich direkt in die Kapitel selbst)

- Danach ggf. dann ein Kapitel über das Deployment

- Irgendwo beim Algotirhmus die "Schnittstelle" mit bpmnXML rein und JSON mit liste der kritischen Elemente (Evtl. mit Begründung) wieder. Durch diese vereinheitlichung ist es möglich verschiedene Algorithmen in dem Evaluationsframework zu vergleichen (Stichwort Standardisierung). Vielleicht gehe ich auch soweit, dass ich den Standard für zukünftige Arbeiten propose, damit spätere Arbeiten meinen Standard zum Vergleich nutzen können

- ??? Soll ich irgendwo noch klar die Abgrenzung einbauen, dass ich hier den einen Algorithmus zur Klassifizierung habe und mithilfe von dem die Modelle vergleiche. Das Evaluationsframework ermöglicht jedoch auch einen vergleich verschiedener Algorithmen. Verschiedene Algorithmen zu vergleichen könnte etwas für eine zukünftige Arbeit sein, kann ich in den Ausblick schreiben. Mit dem Framework ist dafür die Grundlage geschaffen.

- TODO Ich brauche noch einen Seed für reproduzierbare Klassifizierung. Der Seed soll dann auch im Evaluation Report vorhanden sein

TODO: Brauch ich das?
**5.5 Fehlertoleranz & Logging.** Fehlerklassen (Timeout, HTTP 5xx/4xx, Invalid‑JSON, Parse‑Error) werden getrennt gezählt. Berichtet werden zwei Sichtweisen: (a) exkl. Errors (nur valide Antworten) und (b) inkl. Errors (als Fail), um Stabilität fair zu bewerten.

- Wenn ich am Ende noch Zeit habe will ich noch die Erklärungen aus dem Labeling in den Ergebnissen der Testcases anzeigen, damit man zusätzlich noch abgleichen kann, ob das LLM aus den gleiche Gründen die Aktivität gelabelt hat.

- Qualitätssicherung der Labels: Soll ich nochmal wen anders die Prozesse labeln lassen (Cohen's k)

- Retry Menge in der Evaluations Konfig, also wenn ein Testfall bei einem Modell fehlgeschlagen ist, wie oft soll dann retried werden. Das kann dann ja auch eine neue Metrik in den Ergebnissen werden, wie oft ein Modell für einen Testfall gebraucht hat

- Soll ich Baseline noch einbauen (Klassifizierung ohne großes Prompt Engineering (Zero Shot))

- Sind Kosten relevant? Sind Tokens relevant? Ist **Laufzeit** relevant (Ich glaube das wäre geil)?

- Bei 3.2 Qualitätsziele: Zielgrößen konkret (z. B. Recall ≥ 0.85, FPPP ≤ 1.5). Versprich es nicht, deklarier es als Ziel/Messlatte.

- In Diskussion "Claim → Evidence → Caveat → Next." nutzen