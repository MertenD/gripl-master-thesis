\section{Large Language Models (LLMs)}\label{sec:llms}

\acp{LLM} sind große, vortrainierte Sprachmodelle, die auf der Transformer-Architektur basieren. Transformer, erstmals von Vaswani et al. \cite{vaswani2017attention} beschrieben, verarbeiten eine textuelle Eingabe nicht strikt sequenziell, sondern beachten alle Tokens einer Sequenz parallel. Über sogenannte \texttt{Self-Attention} gewichten sie, welche Token füreinander relevant sind. Als Token gelten Wörter oder Wortbestandteile, in die der Text vorab zerlegt wird. Dieser Attention-Mechanismus erfasst Abhängigkeiten über große Distanzen innerhalb der Sequenz und ermöglicht dadurch eine effiziente Kontextmodellierung, was das zentrale Prinzip moderner \acp{LLM} darstellt. Die Transformer-Architektur bildet heute das Fundament moderner Sprachmodelle wie der GPT-Familie von OpenAI \cite{ibm-gpt, minaee2025largelanguagemodelssurvey, openai-models}, die durch ChatGPT breite Anwendung finden.

In chatbasierten Systemen wird das Verhalten des \ac{LLM} über System- und User-Prompts gesteuert. Gutes Prompt Engineering kann die Leistung und Format-Treue der Ausgabe verbessern, ohne dass die Modellparameter verändert werden müssen \cite{liu2023prompting}. Ein deutlicher Vorteil aktueller \acp{LLM} ist Zero-/Few-Shot Learning. Damit lassen sich Aufgaben allein über Instruktionen und wenige Beispiele lösen, ohne dass erneutes Training benötigt wird \cite{brown2020fewshot, liu2023prompting}. Das ist besonders nützlich für Klassifikationsaufgaben, bei denen nur wenige gelabelte Beispiele vorliegen, wie etwa die Identifikation von \ac{DSGVO}-kritischen Aktivitäten in Prozessmodellen.

Um \acp{LLM} in automatisierten Pipelines zu integrieren, sind schema-konforme Ausgaben, wie ein gültiges JSON, unerlässlich. In der Praxis gibt es dafür drei Ansätze:

\begin{enumerate}
    \item Klare Angaben über das Ausgabeformat im System- oder User-Prompt \cite{liu2023prompting}.
    \item API-gestützte Mechanismen wie Function Calling oder Structured-Output/\linebreak~JSON-Mode mit Schemaüberprüfung \cite{mistralai_structured_output, openai_function_calling_2023, openai_structured_output}.
    \item Constrained Decoding, das die Generierung auf eine vorgegebene Grammatik beschränkt. Ein Beispiel ist PICARD: Bei jedem Generationsschritt des \ac{LM} werden nur zulässige Tokens ausgewählt \cite{scholak2021picard}.
\end{enumerate}

Typische Fehlerbilder bei der Nutzung von \acp{LLM} sind Halluzinationen. Diese sind plausibel wirkende, aber fehlerhafte Aussagen und Formatfehler, wie z.\,B. ungültiges JSON. In \cite{kalai2025languagemodelshallucinate} wird argumentiert, dass Halluzinationen bereits beim Erstellen des \ac{LLM} durch die Trainings- und Evaluationsmethoden begünstigt werden, die das Modell dazu bringen, eher zu raten als Unsicherheit zuzugeben. Das Raten bei Unsicherheit verbessert die Testergebnisse. Gegenmaßnahmen gegen Halluzinationen sind u.a. präzisere Prompts, Informationserweiterung des Prompts durch \ac{RAG} und Self-Check/Retry-Strategien als Post-Processing Methoden nach der Generierung \cite{ji2023hallucinationsurvey}.

Die meisten großen \acp{LLM} werden von Unternehmen wie OpenAI, Google oder Anthropic entwickelt und als API-Dienste angeboten. In der Industrie zählt \texttt{GPT-4o} aktuell zu den am weit verbreitetsten Modellen \cite{openai-hello-gpt-4o}. Es ist ein multimodales Modell mit starken Text-, Bild- und Audiofähigkeiten. Proprietäre Modelle wie GPT-4o sind leistungsfähig, bringen jedoch mehrere Nachteile mit sich. Dazu zählen hohe Kosten und mangelnde Transparenz. Außerdem erfolgt die Datenverarbeitung serverseitig auf Infrastruktur der Anbieter, die sich teils außerhalb der \ac{EU} befindet, wo die \ac{DSGVO} nicht gilt. Für die Verarbeitung personenbezogener Daten innerhalb der \ac{EU} ist das problematisch. Eine Übermittlung in Drittländer ist nur zulässig, wenn dort der Auftragsverarbeiter sämtliche Vorgaben aus Kapitel~5 (Art.~44-50) der \ac{DSGVO} einhält \cite{GDPR2016}.

Als Alternative zu proprietären Modellen steht eine wachsende Zahl frei verfügbarer Open-Source-\acp{LLM} zur Verfügung, die auch lokal betrieben werden können. Prominente Beispiele sind die Modelle von Mistral \cite{mistralai}, Deepseek \cite{deepseek} und Qwen \cite{qwen}. Der lokale Betrieb ermöglicht volle Kontrolle darüber, wo und wie Daten verarbeitet werden. Das erleichtert die Einhaltung datenschutzrechtlicher Anforderungen. Zudem bieten Open-Source-Modelle weitere Vorteile wie geringere Kosten und hohe Anpassbarkeit. In dieser Arbeit werden sowohl proprietäre als auch Open-Source-\acp{LLM} evaluiert (siehe Kapitel \ref{ch:modellauswahl}).