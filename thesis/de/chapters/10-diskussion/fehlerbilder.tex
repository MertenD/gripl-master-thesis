\section{Fehlerbilder und Grenzen}\label{sec:fehlerbilder-und-grenzen}

Die Fallstudien verdeutlichen zwei zentrale Schwachstellen. Erstens führen \emph{implizite Annahmen} zu konservativen, \ac{FP}-lastigen Entscheidungen, wenn entsprechende Hinweise im \ac{BPMN}-Modell nicht explizit modelliert sind. Das war beispielsweise bei der Anonymisierung von Klickraten im Testfall \enquote{Marketing-Kampagne} der Fall. Zweitens ist die \emph{Kontextverfolgung über Aktivitätsketten} nicht immer zuverlässig: Weiterverarbeitungen, die sich aus zuvor erhobenen Daten ergeben, werden vereinzelt nicht erkannt, was zu \ac{FN} führt. Dies zeigte sich etwa im Testfall \enquote{Route berechnen}, in dem personenbezogene Standortdaten aus einer vorherigen Aktivität an die Routenberechnung explizit weitergegeben wurden, ohne dass das Modell dies als kritisch erkannte. In der Praxis lassen sich daraus konkrete Gegenmaßnahmen ableiten: Zum einen sollten \ac{BPMN}-Modelle Datenflüsse sichtbarer machen, indem wichtige Elemente für den Kontext wie Datenobjekte/-speicher, Datenassoziationen, Nachrichtenflüsse, Annotationen sowie Pools/Lanes klar modelliert werden. Zum anderen sollten die Prompting-Strategien weiterentwickelt werden, um Modelle explizit auf die Bedeutung von Datenflüssen und Kontextinformationen hinzuweisen. Dazu zählt auch die Überprüfung, ob das Preprocessing der Klassifizierungspipeline ausreichend Kontext bereitstellt oder ob hier Anpassungen notwendig sind.

Die Generalisierbarkeit der Ergebnisse ist durch die Anzahl der Testfälle, deren Annotationen sowie die Auswahl der Modelle begrenzt. Zwar decken die 25 Testfälle eine breite Palette typischer \ac{DSGVO}-kritischer Szenarien ab, doch können sie nicht alle denkbaren Prozessvarianten und -kontexte repräsentieren. Künftige Studien sollten den Datensatz erweitern, um eine größere Vielfalt an Prozessen, Branchen und Komplexitätsgraden abzubilden. Dafür wurde in dieser Arbeit ein Labeling-Tool entwickelt. Zudem könnten weitere \acp{LLM} evaluiert werden, insbesondere solche mit neuen Architekturen oder Trainingsansätzen, um die Vergleichbarkeit zu erhöhen. Hinzu kommen technische Grenzen: Sehr große \ac{BPMN}-Prozesse können die Token-Limits der Modelle überschreiten, was eine Anpassung der Pipeline erfordern würde, etwa durch Prozesssegmentierung. Das Preprocessing verringert bereits die Token-Anzahl, doch sind weitere Optimierungen denkbar.

Schließlich werden die Ergebnisse durch die spezifischen Prompting-Strategien in der Klassifizierungspipeline beeinflusst. So ist etwa im genutzten System-Prompt verankert, dass die Klassifikation bei unklaren Aktivitäten eher konservativ ausfallen soll. Alternative Ansätze könnten zu unterschiedlichen Ergebnissen führen, vor allem mit Blick auf die Balance zwischen \ac{FP} und \ac{FN}. Künftige Arbeiten sollten verschiedene Prompting-Techniken und Pipeline-Designs vergleichen, um die bestmögliche Leistung zu erzielen. Die Leistungsbewertung könnte zudem domänenabhängig sein: In anderen Bereichen bestehen möglicherweise nicht so strenge Anforderungen an Recall und Precision wie im \ac{DSGVO}-Kontext. Abschließend ist zu betonen, dass die Klassifikation keine qualifizierte rechtliche Prüfung ersetzt, sondern als unterstützendes Werkzeug zur Risikominimierung dient.