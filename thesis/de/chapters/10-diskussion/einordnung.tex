\section{Einordnung und Interpretation}\label{sec:einordnung-und-interpretation}

Die Experimente zeigen, dass moderne \acp{LLM} \ac{DSGVO}-kritische Aktivitäten in\linebreak~\ac{BPMN}-Prozessen zuverlässig identifizieren können. Neun von dreizehn Modellen erreichen den angestrebten F1-Score von $\geq 0{,}80$ und erfüllen damit die in Abschnitt \ref{sec:qualitatsziele} definierten Qualitätsziele für ein wirksames Screening, das \emph{\ac{FN}} minimiert, ohne den nachfolgenden Prüfaufwand durch \emph{\ac{FP}} unverhältnismäßig zu erhöhen. Diese Priorisierung auf den \emph{Recall} ist sachlich begründet, da übersehene kritische Aktivitäten schwerwiegende rechtliche und finanzielle Konsequenzen nach sich ziehen können. Im Gegensatz dazu erhöhen \ac{FP} zwar den Prüfaufwand, führen aber nicht zu direkten Verstößen gegen die \ac{DSGVO}. Die Resultate zeigen ein heterogenes Bild: \texttt{GPT-4o} erzielt die höchste Precision mit $0{,}892$, verfehlt jedoch das Recall-Ziel. Umgekehrt erreicht \texttt{Gemma-3-27B-it} einen sehr hohen Recall von $0{,}916$, wird aber durch eine niedrige Precision von $0{,}687$ ausgebremst. Modelle wie \texttt{Qwen3-235B-A22B-Thinking-2507}, \texttt{GPT-OSS-20B} und \texttt{DeepSeek-R1-\linebreak~Distill-Qwen-14B} bieten eine ausgewogene Balance und zählen deshalb zu den Spitzenreitern. Die aggregierten Metriken in Abbildung \ref{fig:results-evaluation-metrics-comparison} und Tabelle \ref{tab:metrics-overview} bestätigen diese Einschätzung.

Ein Teil der Testfälle wurde formal als fehlerhaft gewertet, obwohl alle kritischen Aktivitäten korrekt klassifiziert worden sind. Es wurden jedoch zusätzliche Aktivitäten als kritisch markiert (\ac{FP}) und mit plausiblen Begründungen versehen. Für ein Risiko-Vorscreening ist dieses Verhalten akzeptabel und sogar wünschenswert, solange die \ac{FP}-Last den manuellen Prüfaufwand nicht unverhältnismäßig erhöht. Gleichzeitig zeigt dieses Muster die Notwendigkeit robuster Testdaten auf, die belastbare Labels enthalten: Statt Datensätze nachträglich an Modellausgaben anzupassen, empfiehlt sich für künftige Benchmarks ein Labeling mit mehreren unabhängigen Gutachtern, um subjektive Interpretationen zu minimieren und Grenzfälle klar zu definieren.