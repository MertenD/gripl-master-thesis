\section{Robustheit}\label{sec:robustheit2}

Die Robustheitsanalyse über mehrere Seeds unterstreicht die Praxistauglichkeit der meisten Modelle in Kombination mit der entwickelten Klassifizierungspipeline. Für die Mehrzahl der \acp{LLM} liegt die Standardabweichung des F1-Scores über fünf Wiederholungen bei $\leq 0{,}02$, was auf eine geringe Varianz und konsistente Leistung hinweist. Vereinzelt zeigen Modelle eine höhere Varianz oder benötigen mehr Wiederholungen zur Korrektur von Parsing-Fehlern. Solche Unterschiede sind für den operativen Einsatz relevant, da sie sich direkt in Durchsatz, Latenz und Stabilität der Gesamtpipeline niederschlagen. Modelle mit erhöhter Varianz sollten daher im produktiven Betrieb sorgfältig überwacht und validiert werden, um unerwartete Leistungseinbußen zu vermeiden.

Wesentlich zur Zuverlässigkeit trägt die entwickelte Klassifizierungspipeline bei. \emph{Structured Output} via Langchain4j mit explizitem JSON-Schema (und, wo verfügbar, API-seitig erzwungenem \texttt{response\_format}) erhöht die Format-Treue, ein explizites \texttt{isRelevant}-Flag mit nachgelagertem Relevanz-Filter entschärft Widersprüche zwischen Begründung und Klassifikation, die \emph{id-Validierung/-Vervoll-\linebreak~ständigung} reduziert typische Ausgabefehler und der \emph{Retry-Mechanismus} behebt Parsing-Fehler automatisiert. Diese Maßnahmen tragen wesentlich zur Ergebnisstabilität bei und sollten in produktiven Systemen implementiert werden, um die Zuverlässigkeit der Modellausgaben zu gewährleisten.

Ob das Preprocessing der Klassifizierungspipeline zur Leistung beiträgt, lässt sich nicht abschließend beurteilen. Die im nächsten Abschnitt beschriebenen Fallstudien legen nahe, dass trotz des im Preprocessing bereitgestellten Kontexts Datenflüsse und Prozesszusammenhänge durch \ac{LLM} weiterhin unberücksichtigt bleiben oder falsch interpretiert werden. Hier könnten künftige Anpassungen der Pipeline ansetzen, um den Kontext für die Modelle weiter zu verbessern.