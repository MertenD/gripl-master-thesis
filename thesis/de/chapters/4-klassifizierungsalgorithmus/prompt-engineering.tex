\section{Prompt Engineering}\label{sec:prompt-engineering}

Eine robuste Klassifikation hängt maßgeblich von sorgfältig gestalteten Prompts ab. Ziel ist es, das \ac{LLM} mit klaren Anweisungen, einem konsistenten Bewertungsschema und präzisen Formatvorgaben so zu steuern, dass es die Klassifizierung zuverlässig löst und strukturierte Ausgaben liefert. Im Folgenden werden zunächst die deklarative Orchestrierung der Kommunikation mit dem \ac{LLM} mithilfe von LangChain4j und anschließend die Prompt-Konzeption beschrieben.

\subsection*{LangChain4j: deklarative Orchestrierung}
Zur Reduktion von Boilerplate und für konsistente Prompts wird \emph{LangChain4j} \cite{langchain4j} benutzt. Mit den \emph{AI Services} werden Interaktionen mit dem \ac{LLM} als Java/Kotlin-Interface \emph{deklarativ} beschrieben. Zur Laufzeit erzeugt LangChain4j einen Proxy, der den System-Prompt injiziert, den User-Prompt aus den Methodenparametern generiert und die \ac{LLM}-Antwort in den passenden Rückgabetyp deserialisiert \cite{langchain4j-ai-services}. Beim erstellen des AI Service werden ein \texttt{ChatModel}, die Systemnachricht und die Interface-Methoden konfiguriert. Ein \texttt{ChatModel} ist die spezifische Implementierung der Chat-Completion-Schnittstelle eines \ac{LLM} von Langchain4j \cite{langchain4j-chat-model}. Die Methodenparameter der Interface-Methoden repräsentieren die Nutzereingabe. Der Rückgabetyp der Methoden definiert die erwartete Antwortstruktur des \ac{LLM}. Optional kann jeder Interface-Methode noch ein eigener User-Prompt zugewiesen werden, der bei Laufzeit mit den übergebenen Parametern gefüllt wird \cite{langchain4j-ai-services}.

Die Kommunikation mit dem \ac{LLM} erfolgt damit über einfache Funktionsaufrufe, während LangChain4j Prompt-Erzeugung, Parameterbindung sowie die Deserialisierung der Antwort übernimmt \cite{langchain4j-ai-services}. So kann im Code ohne zusätzlichen Aufwand direkt mit typisierten Objekten gearbeitet werden.

Für die Klassifikation \emph{\ac{DSGVO}-kritisch vs.\ unkritisch} wird ein \textbf{Zero-Shot}-Ansatz verwendet. Das \ac{LLM} erhält im System-Prompt eine präzise Instruktion mit Kriterien und illustrativen Beispielfällen, was als kritisch gilt. Es sind jedoch keine Beispiele mit konkreten Ein- und Ausgabe-paaren pro Prozess enthalten. Zero-Shot reduziert den Pflegeaufwand und nutzt die In-Context-Fähigkeiten moderner Modelle, nur über Instruktionen zu generalisieren \cite{brown2020fewshot,liu2023prompting}. Wie genau die Prompts aufgebaut sind, wird im Folgenden beschrieben.

\subsection*{System-Prompt}

Der System-Prompt definiert das Verhalten des \ac{LLM}, zusätzlichen Kontext und das gewünschte Ausgabeformat. Der vollständige System-Prompt befindet sich im Anhang, siehe \ref{lst:system-prompt}. Im Kern legt der genutzte System-Prompt Folgendes fest:

\begin{enumerate}
    \item \textbf{Rolle und Auftrag des Modells.} Das Modell agiert als Experte für das Analysieren von \ac{BPMN}-Modellen auf \ac{DSGVO}-konformität und prüft sämtliche Aktivitäten eines Prozesses auf Datenschutzrelevanz. Jede Aktivität wird berücksichtigt und die Entscheidung erfolgt auf Basis sämtlicher verfügbarer Kontextinformationen wie Name, Beschreibung, Annotationen sowie Daten- und Nachrichtenassoziationen.
    \item \textbf{Rechtliche Definitionen nach \ac{DSGVO}.} Der System-Prompt erläutert die Begriffe \enquote{personenbezogene Daten} und \enquote{Verarbeitung} gemäß Art.\,4 \ac{DSGVO}. Beispiele für personenbezogene Daten umfassen Identifikatoren, Kontakt- und Zahlungsdaten, Beschäftigungsdaten, Gesundheitsdaten, biometrische Merkmale, Standortinformationen und Online-Kennungen. Verarbeitung umfasst Erheben, Speichern, Abrufen, Verwenden, Übermitteln, Ausrichten, Kombinieren, Einschränken, Löschen und Vernichten.
    \item \textbf{Indikatoren für Kritikalität.} Der System-Prompt enthält typische Auslöser für Datenschutzrelevanz wie Datenerfassung und Dateneingabe, Anlage und Aktualisierung von Datensätzen, Übermittlung oder Offenlegung an andere Systeme oder Dritte, Zahlungen und Finanztransaktionen und noch mehr. Diese Indikatoren sind mit Beispielen angereichert und dienen als \emph{Entscheidungshelfer} für das Modell.
    \item \textbf{Abgrenzung durch Negativbeispiele.} Der System-Prompt grenzt unkritische Fälle klar ab. Rein administrative oder logistische Schritte ohne Personenbezug werden nicht als kritisch gewertet. Ebenso gilt dies für Fälle in denen anonymisierte Daten verwendet werden und keine Identifikation einer Person mehr möglich ist.
    \item \textbf{Erwartetes Ausgabeformat.} Die Antwort erfolgt als strukturierte JSON-\hspace{0pt}Ausgabe mit einer Liste relevanter Aktivitäten. Für jede Aktivität wird die \texttt{id} und eine Begründung in natürlicher Sprache ausgegeben. Es werden ausschließlich Aktivitäten zurückgegeben, die nach den Kriterien als datenschutzrelevant eingestuft wurden.
\end{enumerate}

Die Kombination dieser Elemente im System-Prompt stellt sicher, dass das \ac{LLM} die Aufgabe versteht, die relevanten Kriterien kennt und die Ausgabe in einem maschinenlesbaren Format liefert. So entsteht die Basis für eine zuverlässige Klassifikation. Zu einer Anfrage an ein \ac{LLM} gehört außerdem stets ein User-Prompt, der die eigentliche Nutzereingabe enthält. Dessen Aufbau wird im nächsten Abschnitt beschrieben.

\subsection*{User-Prompt}

Der User-Prompt übergibt dem \ac{LLM} die konkreten Eingabedaten einer Anfrage. Während der System-Prompt Regeln, Ziele und Ausgabevorgaben festlegt, liefert der User-Prompt die Fall- bzw.\ Kontextinformationen, auf die diese Regeln angewendet werden.

Der User-Prompt wird mithilfe der Daten aus der Vorverarbeitung aus Abschnitt \ref{sec:bpmn-preprocessing} erzeugt und enthält eine Liste von \texttt{BpmnElement}-Objekten, siehe Listing~\ref{lst:bpmn-element-class}. Die Interaktion mit dem \ac{LLM} erfolgt deklarativ über \emph{LangChain4j}. Dafür wird die Liste der \texttt{BpmnElement}-Objekte als Methodenparameter mit der Annotation \texttt{@UserMessage} an die Interface-Methode übergeben und dort automatisch in den User-Prompt eingebettet.

Zur Laufzeit serialisiert \emph{LangChain4j} die \texttt{BpmnElement}-Liste zu einem JSON-Array und stellt sie als User-Prompt bereit. Der zuvor konfigurierte System-Prompt wird bei einer Anfrage an das \ac{LLM} automatisch dem User-Prompt vorangestellt. Auf diese Weise wendet das \ac{LLM} die im System-Prompt definierten Kriterien auf die im User-Prompt gelieferten Informationen zum \ac{BPMN}-Prozessmodell an. Dadurch wird jede Aktivität des Prozesses genau so wie im System-Prompt beschrieben klassifiziert. In Abbildung \ref{fig:architecture-diagram} findet dieser Schritt in der Aktivität \enquote{Anfrage an das LLM schicken} statt.

Zusammenfassend setzt der System-Prompt typischerweise das Regelwerk, und der User-Prompt liefert die konkreten Eingabedaten. Besonders in mehrstufigen Dialogen mit dem \ac{LLM} spielt dieses Muster eine größere Rolle, da der System-Prompt konstant bleibt, während der User-Prompt je nach Anfrage variiert. Im vorliegenden Szenario, wo immer nur genau eine Anfrage pro Prozessmodell gestellt wird fällt der Unterschied weniger ins Gewicht, als würden sämtliche Vorgaben direkt im User-Prompt stehen. Die Trennung erhöht dennoch die Nachvollziehbarkeit, sorgt für klare Rollen und erleichtert die Wiederverwendung.

Im folgenden Abschnitt wird beschrieben, wie auf dieser Basis strukturierte Ausgaben erzeugt werden, damit im Code direkt mit typisierten Objekten weitergearbeitet werden kann.

\subsection*{Strukturierte Ausgaben mit LangChain4j}

Im Fall der Klassifikation wird ein \texttt{BpmnAnalysisResult} als Antwort erwartet, also eine Liste von Elementen mit Paaren aus \texttt{id}, \texttt{reason} und \texttt{isRelevant}. Siehe \ref{lst:bpmn-analysis-result} für die vollständige Definition der Datenklasse. Langchain4j inferiert auf Basis des Rückgabetyps der Interface-Methode ein JSON-Schema und fügt dieses automatisch dem User-Prompt zusammen mit der Aufforderung, die Antwort in diesem JSON-Format zu liefern hinzu \cite{langchain4j-ai-services}. Durch die explizite Angabe des gewünschten JSON-Formats im Prompt wird die Format-Treue der Antwort, also die Wahrscheinlichkeit, dass die Antwort tatsächlich dem gewünschten Schema entspricht, erhöht \cite{liu2023prompting}.

Einige \acp{LLM} unterstützen darüber hinaus die Möglichkeit, das Antwortformat API-seitig zu erzwingen. Das ist beispielsweise bei Mistral und OpenAI der Fall \cite{mistralai_structured_output, openai_structured_output}.
Falls das \ac{LLM} die \texttt{response\_format} Funktionalität unterstützt, setzt LangChain4j dies zusätzlich auf das gewünschte Schema und erzwingt so das Ziel-JSON API-seitig \cite{langchain4j-ai-services}. Fehlt diese Fähigkeit, greift ausschließlich die Prompt-basierte Schemaanweisung.

Das vom \ac{LLM} gelieferte JSON deserialisiert \emph{LangChain4j} anschließend automatisch zu einem \texttt{BpmnAnalysisResult}. So kann im Code direkt mit einem typsicheren Objekt weitergearbeitet werden. In Abbildung \ref{fig:architecture-diagram} ist dieser Prozess über die Aktivitäten \enquote{Antwort von LLM erhalten} und \enquote{Antwort JSON deserialiseren} dargestellt.
