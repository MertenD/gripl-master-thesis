\section{Prompt Engineering}\label{sec:prompt-engineering}

// TODO Irgendwo hier will ich noch das isRelevant im BpmnAnalysisResult ansprechen, dass damit die Ergebnisse besser wurden und alles was isRelevant false ist, wird vorm ausgeben herausgefiltert. Besonders interessant fand ich die Beobachtung, dass das Modell trotz der genauen Beschreibung, nur kritische Elemente auszugeben, trotzdem oft unkritische Elemente mit ausgegeben hat. Mit dem isRelevant Flag konnte ich das aber gut filtern.

Eine robuste Klassifikation hängt maßgeblich von sorgfältig gestalteten Prompts ab. Ziel ist es, das \ac{LLM} mit klaren Anweisungen, einem konsistenten Bewertungsschema und präzisen Formatvorgaben so zu steuern, dass es die Klassifizierung zuverlässig löst und strukturierte Ausgaben liefert. Im Folgenden werden zunächst die deklarative Orchestrierung der Kommunikation mit dem \ac{LLM} mithilfe von LangChain4j und anschließend die Prompt-Konzeption beschrieben.

\subsection*{LangChain4j: deklarative Orchestrierung}
Zur Reduktion von Boilerplate und für konsistente Prompts wird \emph{LangChain4j} benutzt \cite{langchain4j}. Mit den \emph{AI Services} werden Interaktionen mit dem \ac{LLM} als Java/Kotlin-Interface \emph{deklarativ} beschrieben. Zur Laufzeit erzeugt LangChain4j einen Proxy, der den System-Prompt injiziert, den User-Prompt aus den Methodenparametern generiert und die \ac{LLM}-Antwort in den passenden Rückgabetyp deserialisiert \cite{langchain4j-ai-services}. Beim erstellen des AI Service werden ein \texttt{ChatModel}, die Systemnachricht und die Interface-Methoden konfiguriert. Ein \texttt{ChatModel} ist von Langchain4j die spezifische Implementierung der Chat-Completion-Schnittstelle eines \ac{LLM} \cite{langchain4j-chat-model}. Die Methodenparameter der Interface-Methoden repräsentieren die Nutzereingabe. Der Rückgabetyp der Methoden definiert die erwartete Antwortstruktur des \ac{LLM}. Optional kann jeder Interface-Methode noch ein eigener User-Prompt zugewiesen werden, der bei Laufzeit mit den übergebenen Parametern gefüllt wird \cite{langchain4j-ai-services}.

Die Kommunikation mit dem \ac{LLM} erfolgt damit über einfache Funktionsaufrufe, während LangChain4j Prompt-Erzeugung, Parameterbindung sowie die Deserialisierung der Antwort übernimmt \cite{langchain4j-ai-services}. So kann im Code ohne zusätzlichen Aufwand direkt mit typisierten Objekten gearbeitet werden.

\subsection*{Zero-Shot mit eingebetteten Kategorien}

Für die Klassifikation \emph{\ac{DSGVO}-kritisch vs.\ unkritisch} wird ein \textbf{Zero-Shot}-Ansatz verwendet. Das \ac{LLM} erhält im System-Prompt eine präzise Instruktion mit Kriterien- und illustrativen Beispielfällen, was als kritisch gilt. Es sind jedoch keine Beispiele mit konkreten Eingaben- und Ausgaben-paaren pro Prozess enthalten. Zero-Shot reduziert den Pflegeaufwand und nutzt die In-Context-Fähigkeiten moderner Modelle, nur über Instruktionen zu generalisieren \cite{brown2020fewshot,liu2023prompting}. Wie genau die Prompts aufgebaut sind, wird im Folgenden beschrieben.

\subsection*{System-Prompt}

Der System-Prompt definiert das Verhalten des \ac{LLM}, zusätzlichen Kontext und das gewünschte Ausgabeformat. Der vollständige System-Prompt befindet sich im Anhang, siehe \ref{lst:system-prompt}. Im Kern legt der genutzte System-Prompt Folgendes fest:

\begin{enumerate}
    \item \textbf{Rolle und Auftrag des Modells.} Das Modell agiert als Experte für das Analysieren von \ac{BPMN}-Modellen auf \ac{DSGVO}-konformität und prüft sämtliche Aktivitäten eines Prozesses auf Datenschutzrelevanz. Jede Aktivität wird berücksichtigt und die Entscheidung erfolgt auf Basis sämtlicher verfügbarer Kontextinformationen wie Name, Beschreibung, Annotationen sowie Daten- und Nachrichtenassoziationen.
    \item \textbf{Rechtliche Definitionen nach \ac{DSGVO}.} Der System-Prompt erläutert die Begriffe \enquote{personenbezogene Daten} und \enquote{Verarbeitung} gemäß Art.\,4 \ac{DSGVO}. Beispiele für personenbezogene Daten umfassen Identifikatoren, Kontakt- und Zahlungsdaten, Beschäftigungsdaten, Gesundheitsdaten, biometrische Merkmale, Standortinformationen und Online-Kennungen. Verarbeitung umfasst Erheben, Speichern, Abrufen, Verwenden, Übermitteln, Ausrichten, Kombinieren, Einschränken, Löschen und Vernichten.
    \item \textbf{Indikatoren für Kritikalität.} Der System-Prompt enthält typische Auslöser für Datenschutzrelevanz wie Datenerfassung und Dateneingabe, Anlage und Aktualisierung von Datensätzen, Übermittlung oder Offenlegung an andere Systeme oder Dritte, Zahlungen und Finanztransaktionen und noch mehr. Diese Indikatoren sind mit Beispielen angereichert und dienen als \emph{Entscheidungshelfer} für das Modell.
    \item \textbf{Abgrenzung durch Negativbeispiele.} Der System-Prompt grenzt unkritische Fälle klar ab. Rein administrative oder logistische Schritte ohne Personenbezug werden nicht als kritisch gewertet. Ebenso gilt dies für Fälle in denen anonymisierte Daten verwendet werden und keine Identifikation einer Person mehr möglich ist.
    \item \textbf{Erwartetes Ausgabeformat.} Die Antwort erfolgt als strukturierte JSON-Ausgabe mit einer Liste relevanter Aktivitäten. Für jede Aktivität wird die \texttt{id} und eine Begründung in natürlicher Sprache ausgegeben. Es werden ausschließlich Aktivitäten zurückgegeben, die nach den Kriterien als datenschutzrelevant eingestuft wurden.
\end{enumerate}

\subsection*{User-Prompt}

Der User-Prompt wird aus der Vorverarbeitung in \ref{sec:bpmn-preprocessing} erzeugt und enthält eine Liste von \texttt{BpmnElement}-Objekten, siehe Listing~\ref{lst:bpmn-element-class}. Der User-Prompt wird automatisch durch \emph{Langchain4j} generiert, indem die Liste der \texttt{BpmnElement}-Objekte als Argument an die Interface-Methode übergeben wird, wie in Listing~\ref{lst:ai-service-interface} zu sehen.

\begin{lstlisting}[language=Kotlin,caption={Deklarative Schnittstelle für die Analyse eines \ac{BPMN}-Prozesses.},label={lst:ai-service-interface}]
fun analyze(@UserMessage bpmnElements: Set<BpmnElement>): BpmnAnalysisResult
\end{lstlisting}

Die Liste der \texttt{BpmnElement}-Objekte wird intern zu einem JSON-Array serialisiert und als User-Prompt an das \ac{LLM} übergeben. Der System-Prompt wird dabei automatisch vorangestellt und sorgt dafür, dass der User-Prompt korrekt interpretiert wird.

\subsection*{Strukturierte Ausgaben mit LangChain4j}

Wie in Listing \ref{lst:ai-service-interface} zusehen, wird im Fall der Klassifikation ein \texttt{BpmnAnalysisResult} als Antwort erwartet, also eine Liste von Elementen mit \texttt{id}, \texttt{reason}. Siehe \ref{lst:bpmn-analysis-result} für die vollständige Definition der Datenklasse. Langchain4j erstellt auf Basis des Rückgabetyps ein JSON-Schema und fügt dieses dem User-Prompt zusammen mit der Aufforderung hinzu, die Antwort in diesem JSON-Format zu liefern.

Falls das \texttt{ChatModel} die \texttt{response\_format} Funktionalität unterstützt - wie es bei Mistral und OpenAI der Fall ist \cite{mistralai_structured_output, openai_structured_output} - setzt LangChain4j dies zusätzlich auf das gewünschte Schema und erzwingt so das Ziel-JSON API-seitig \cite{langchain4j-ai-services}. Fehlt diese Fähigkeit, greift ausschließlich die Prompt-basierte Schemaanweisung.

Durch die explizite Angabe des Ziel-JSON im Prompt wird die Format-Treue der Antwort erhöht \cite{liu2023prompting}. Das erhaltene JSON wird anschließend automatisch in ein \texttt{BpmnAnalysisResult}-Objekt deserialisiert, sodass im Code direkt mit dem typisierten Objekt weitergearbeitet werden kann.