\section{Evaluationsergebnisse}\label{sec:generierte-resultate}

\textcolor{orange}{// TODO Hier brauche ich einen guten Einleitungs/Überleitungssatz/Abschnitt}

Für jeden Testfall eines Modells liegen vor: die von der Klassifizierungs-Pipeline zurückgegebenen klassifizierten Aktivitäten (mit optionalen Begründungen), die gelabelten erwarteten Aktivitäten, die Zählwerte für \emph{\ac{TP}}, \emph{\ac{FP}}, \emph{\ac{FN}} und \emph{\ac{TN}} sowie eine Bild-URL zur Visualisierung des \ac{BPMN}-Modells mit hervorgehobenen Aktivitäten. Aus diesen Informationen lässt sich ableiten, ob der Testfall erfolgreich war. Ein Testfall gilt als erfolgreich, wenn die klassifizierten Aktivitäten exakt den erwarteten Aktivitäten entsprechen. Technische Probleme, die während der Klassifizierung auftreten, werden ebenfalls erfasst, z.\,B.\ Parsing-Fehler, ungültiges \ac{BPMN}, Token-Limit-Überschreitungen oder Zeitüberschreitungen.

Auf Modellebene stehen die Gesamtergebnisse über alle Testfälle zur Verfügung. Dazu gehören die aggregierten Kennzahlen \emph{Precision}, \emph{Accuracy}, \emph{Recall} und \emph{F1-Score} sowie eine Konfusionsmatrix mit den Gesamtwerten für \emph{\ac{TP}}, \emph{\ac{FP}}, \emph{\ac{FN}} und \emph{\ac{TN}}. Zusätzlich sind die Anzahlen der korrekt bzw.\ falsch klassifizierten sowie der technisch fehlgeschlagenen Testfälle aufgeführt.

Abschließend sind die Metadaten der gesamten Evaluierung verfügbar. Die verwendeten Testdatensätze, die Anzahl der Testfälle, die konfigurierten Modelle, der für die Reproduzierbarkeit verwendete Seed, sowie ein Zeitstempel der Evaluierung. Zum unmittelbaren Vergleich werden die aggregierten Kennzahlen aller Modelle nebeneinander dargestellt. Alle Ergebnisse können über ein webbasiertes Frontend, das im nächsten Abschnitt beschrieben wird, eingesehen und im Detail analysiert werden.