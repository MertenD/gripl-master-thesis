\section{Beantwortung der Forschungsfragen}\label{sec:antworten-auf-forschungsfragen}

Auf Basis der vorhergehenden Auswertungen lassen sich die in Abschnitt~\ref{sec:zielsetzung-und-beitrage} formulierten Forschungsfragen beantworten. Die folgenden Antworten berücksichtigen sowohl die quantitativen Ergebnisse als auch die qualitative Beobachtungen aus den Fallstudien und ordnen sie unter Berücksichtigung der Qualitätsziele ein.

\paragraph{UF1: Wie gut schneiden europäische Modelle im Vergleich zu internationalen Modellen ab?}

Die europäischen Modelle zeigen eine große Bandbreite in ihrer Leistungsfähigkeit. \texttt{Mistral Medium 3.1} erfüllt mit einem F1-Score $= 0{,}843$, einem Recall $= 0{,}877$ und einer Precision $= 0{,}811$ sämtliche Qualitätsziele und übertrifft das Referenzmodell \texttt{GPT-4o}. \texttt{Mistral-Large-Instruct-2411} erreicht mit einem F1-Score $= 0{,}823$ ebenfalls alle Zielwerte. Dagegen schneiden \texttt{Mistral-\linebreak~7B-Instruct-v0.3} mit F1-Score $= 0{,}777$ und insbesondere \texttt{Mixtral-8x7B-\linebreak~Instruct-v0.1} mit F1-Score $= 0{,}596$ deutlich schlechter ab. Im Durchschnitt liegen die internationalen Modelle - insbesondere die Qwen- und GPT-OSS-Varianten - vor den europäischen und bieten eine robustere Balance aus Recall und Precision. Dennoch zeigen \texttt{Mistral Medium 3.1} und \texttt{Mistral-Large-Instruct-\linebreak~2411}, dass leistungsfähige europäische Alternativen existieren.

\paragraph{UF2: Wie unterscheiden sich große und kleine Modelle in ihrer Leistungsfähigkeit?}

Der direkte Vergleich zeigt, dass sich kleine ($\leq 25$B Parameter) und große Modelle kaum im Durchschnitt ihrer Metriken unterscheiden. Beide Größenklassen erreichen praktisch identische mittlere F1-Scores von etwa $0{,}80$. Ohne das Ausreißermodell \texttt{Mixtral-8x7B-Instruct-v0.1} liegt der Durchschnitt der großen Modelle mit $0{,}836$ zwar etwas höher, doch belegen Modelle wie \texttt{GPT-OSS-\linebreak~20B}, dass kleinere Modelle mit den großen mithalten können. Entscheidend sind Trainingsdaten, Feinabstimmung und Architektur, nicht allein die Parameteranzahl.

\paragraph{UF3: Welche Open-Source-Modelle (insbesondere aus der EU) erzielen die besten Ergebnisse?}

Unter den offenen Modellen dominieren die chinesischen Qwen-Varianten und die GPT-OSS-Modelle. \texttt{Qwen3-235B-A22B-Thinking-2507} erreicht mit einem F1-Score $= 0{,}874$ und einem Recall $= 0{,}932$ die Spitzenposition, gefolgt von \texttt{GPT-OSS-20B} mit F1-Score $= 0{,}866$ und Recall $= 0{,}918$ und \texttt{DeepSeek-R1-Distill-Qwen-14B} mit F1-Score $= 0{,}848$ und Precision $= 0{,}829$. Diese Modelle übertreffen die proprietären Benchmarks deutlich. Das leistungsstärkste offene \ac{EU}-Modell ist \texttt{Mistral-Large-Instruct-2411} mit F1-Score $= 0{,}823$, während \texttt{Mistral-7B-Instruct-v0.3} und \texttt{Mixtral-8x7B-Instruct-\linebreak~v0.1} die Zielwerte verfehlen.

\paragraph{UF4: Wie gut schneiden Open-Source-Modelle gegenüber kommerziellen Modellen wie GPT-4o ab?}

Mehrere Open-Source-Modelle übertreffen die kommerziellen Vertreter. \texttt{Qwen3-235B-A22B-Thinking-2507}, \texttt{GPT-OSS-20B} und\linebreak\texttt{DeepSeek-R1-Distill-Qwen-14B} erreichen höhere F1- und Recall-Werte als sowohl \texttt{GPT-4o} als auch \texttt{Mistral Medium 3.1}. \texttt{GPT-4o} überzeugt mit einer außergewöhnlich hohen Precision von $0{,}892$, verfehlt aber das Recall-Mindestziel. \texttt{Mistral Medium 3.1} bietet einen ausgewogenen Kompromiss und erfüllt alle Zielwerte, liegt aber hinter den besten Open-Source-Modellen. Insgesamt zeigen hochwertige Open-Source-Modelle die beste Balance zwischen hohem Recall und akzeptabler Precision.

Auf Basis der durchgeführten Experimente, Analysen und Antworten auf die Unterfragen lässt sich die Hauptforschungsfrage im Folgenden beantworten.

\paragraph{FF1: Wie zuverlässig identifizieren \acp{LLM} DSGVO-kritische Aktivitäten in\linebreak~BPMN-Prozessmodellen?}

Die überwiegende Mehrheit der Modelle kann kritische Aktivitäten mit hoher Zuverlässigkeit erkennen. Neun von dreizehn Modellen erreichen einen F1-Score von mindestens $0{,}80$ und erfüllen damit den Zielwert. Die Spitzenmodelle \texttt{Qwen3-235B-A22B-Thinking-2507}, \texttt{GPT-OSS-20B}, \texttt{DeepSeek-\linebreak~R1-Distill-Qwen-14B} und \texttt{Mistral Medium 3.1} erzielen F1-Scores zwischen\linebreak$0{,}843$ und $0{,}874$ bei Recall-Werten von $0{,}868$ bis $0{,}932$. Gleichzeitig gibt es Modelle wie \texttt{Mixtral-8x7B-Instruct-v0.1} und \texttt{Qwen2.5-7B-Instruct}, die deutlich abfallen.

Die Robustheitsanalyse zeigt, dass die meisten Modelle, mit einer Standardabweichung der F1-Scores von $\leq 0{,}02$, eine geringe Varianz über verschiedene Seeds aufweisen und häufig keine Retries benötigen, um eine korrekte JSON-Ausgabe zu produzieren. Ausreißer wie \texttt{Mistral Medium 3.1} (höhere Varianz) oder \texttt{Mistral-\linebreak~7B-Instruct-v0.3} (viele Retries) sollten im praktischen Einsatz sorgfältig überprüft werden.

Die Fallstudien unterstreichen, dass \ac{FP} vor allem dann entstehen, wenn im Prozessmodell wichtige Kontextinformationen fehlen, wie z.\,B.\ Anonymisierung von\linebreak~Klickraten, und Modelle daher konservativ entscheiden. \ac{FN} treten auf, wenn Datenflüsse über mehrere Aktivitäten nicht korrekt erkannt werden. Trotz dieser Fehlerbilder zeigen die Experimente, dass \acp{LLM} für ein automatisiertes Screening von \ac{BPMN}-Prozessen sehr gut geeignet sind. Eine nachgelagerte menschliche Prüfung bleibt jedoch sinnvoll, um verbleibende \ac{FP} und \ac{FN} zu adressieren und die Ergebnisse kontextsensitiv zu bewerten.