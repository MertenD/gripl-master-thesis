\section{Antworten auf Forschungsfragen}\label{sec:antworten-auf-forschungsfragen}

Basierend auf den vorstehenden Auswertungen können die in \ref{sec:zielsetzung-und-beitrage} formulierten Forschungsfragen beantwortet werden. Die nachfolgenden Antworten berücksichtigen sowohl die quantitativen Ergebnisse als auch qualitative Beobachtungen aus den Fallstudien und ordnen sie unter berücksichtigung der Qualitätsziele aus Abschnitt \ref{sec:qualitatsziele} ein.

\paragraph{UF1: Wie gut schneiden europäische Modelle im Vergleich zu internationalen Modellen ab?}

Die europäischen Modelle zeigen eine große Spannweite in ihrer Leistungsfähigkeit. Wie in Abschnitt~\ref{sec:qualitatsziele} dargelegt, gelten als Qualitätsziele ein Recall von mindestens $0{,}80$ mit angestrebtem Bereich ab $0{,}85$, eine Precision von mindestens $0{,}75$, ein F1-Score von mindestens $0{,}80$ sowie höchstens $1{,}5$ \acp{FP} je Prozess.

Das proprietäre \texttt{Mistral Medium 3.1} ist mit einem F1-Score von $0{,}843$ und einem Recall von $0{,}877$ das leistungsstärkste \ac{EU}-Modell. Es übertrifft \texttt{GPT-4o} beim F1-Score und beim Recall, liegt jedoch hinter den besten internationalen Modellen. Im Rahmen der Qualitätsziele erfüllt \texttt{Mistral Medium 3.1} alle Kriterien, denn der Recall liegt im angestrebten Bereich, die Precision von $0{,}811$ überschreitet die Untergrenze und der F1-Score liegt über dem Zielwert.

\texttt{Mistral-Large-Instruct-2411} bewegt sich mit Blick auf F1-Score und Recall im Mittelfeld der getesteten Modelle und liegt knapp vor \texttt{GPT-4o}. Die Qualitätsziele werden erreicht, denn der Recall beträgt $0{,}872$ und erreicht das Mindestniveau, die Precision liegt mit $0{,}779$ über der Untergrenze und der F1-Score von $0{,}823$ erfüllt das Ziel.

Das kleine Modell \texttt{Mistral-7B-Instruct-v0.3} erzielt mit F1-Score $0{,}777$ und Recall $0{,}856$ – insbesondere für die Größe – solide Werte und ordnet sich insgesamt im unteren Mittelfeld ein. Das Recall-Ziel wird erreicht, die Precision von $0{,}712$ unterschreitet jedoch die Untergrenze, und der F1-Score bleibt dadurch unter dem Zielwert.

Das \ac{MoE}-Modell \texttt{Mixtral-8x7B-Instruct-v0.1} schneidet mit F1-Score $0{,}596$ und Recall $0{,}550$ deutlich schlechter ab und bildet das Schlusslicht der getesteten Modelle. Sämtliche Zielwerte werden verfehlt, da auch die Precision mit $0{,}652$ unter der Untergrenze liegt.

In jeder Größenklasse finden sich internationale Modelle, die die europäischen Modelle übertreffen. \texttt{Qwen3-235B-A22B-Thinking-2507} erreicht einen Recall von $0{,}932$ und einen F1-Score von $0{,}874$ und liegt damit klar im Zielbereich. \texttt{GPT-OSS-20B} und \texttt{GPT-OSS-120B} erfüllen mit Recall $0{,}918$ beziehungsweise $0{,}906$ und F1-Score $0{,}866$ beziehungsweise $0{,}862$ sämtliche Kriterien. \texttt{GPT-4o} erfüllt mit Precision $0{,}892$ und F1-Score $0{,}822$ die entsprechenden Zielwerte, unterschreitet jedoch das Recall-Mindestniveau mit $0{,}762$.

Damit lässt sich festhalten, dass die internationalen Modelle im Mittel eine höhere Klassifikationsqualität bieten. \ac{EU}-Modelle können vor allem durch ihre datenschutzfreundliche Bereitstellung und ihren Standortvorteil punkten. Leistungsmäßig erfüllen \texttt{Mistral Medium 3.1} und \texttt{Mistral-Large-Instruct-2411} die Qualitätsziele, während \texttt{Mistral-7B-Instruct-v0.3} und \texttt{Mixtral-8x7B-Instruct-\linebreak~v0.1} diese nicht vollständig erreichen.

\paragraph{UF2: Wie unterschieden sich große und kleine Modelle in ihrer Leistungsfähigkeit?}

Die Gegenüberstellung kleiner Modelle mit $\leq 25B$ Parametern und großer Modelle mit $> 25B$ Parametern ergab kaum Unterschiede im Durchschnitt. Der mittlere F1-Score liegt mit $0{,}805$ bei den kleinen und $0{,}806$ bei den großen Modellen praktisch gleichauf. Ohne das deutlich unterdurchschnittliche \texttt{Mixtral-\linebreak~8x7B-Instruct-v0.1} steigt der Durchschnitt der großen Modelle auf $0{,}836$, was zeigt, dass größere Modelle tendenziell bessere Leistungen erbringen können. Die Recall-Werte der kleinen Modelle von durchschnittlich $0{,}843$ übertreffen die der großen Modelle von $0{,}839$ leicht, während Precision und Accuracy in beiden Gruppen vergleichbar sind. Auffällig ist die höhere Varianz der großen Modelle: die Standardabweichung des F1-Scores beträgt $0{,}089$, bei den kleinen lediglich $0{,}057$. Dies ist vor allem auf das Ausreißermodell \texttt{Mixtral-8x7B-Instruct-v0.1} zurückzuführen. Bei den Bestwerten liegen die Gruppen einigermaßen dicht beieinander: das beste kleine Modell \texttt{GPT-OSS-20B} erzielt einen F1-Score von $0{,}866$ und einen Recall von $0{,}918$, während das beste große Modell \texttt{Qwen3-235B-A22B-Thinking-\linebreak~2507} einen F1-Score von $0{,}874$ und einen Recall von $0{,}932$ erreicht.

Die Analyse zeigt, dass die Modellgröße allein kein entscheidender Faktor für die Klassifikationsleistung ist. Vielmehr sind Trainingsdaten, Feinabstimmung und Architektur entscheidender. Leistungsfähige kleine Modelle können mit den großen mithalten und sind im praktischen Einsatz ressourcenschonender.

\paragraph{UF3: Welche Open-Source-Modelle (insbesondere aus der EU) erzielen die besten Ergebnisse?}

Unter den offenen Modellen erzielen \texttt{Qwen3-235B-A22B-\linebreak~Thinking-2507}, \texttt{GPT-OSS-20B} und \texttt{DeepSeek-R1-Distill-Qwen-14B} die besten Ergebnisse. \texttt{Qwen3-235B-A22B-Thinking-2507} erreicht mit einem F1-Score von $0{,}874$, einem Recall von $0{,}932$ und einer Precision von $0{,}824$ den Spitzenplatz. \texttt{GPT-OSS-20B} folgt mit F1 $0{,}866$, Recall $0{,}918$ und Precision $0{,}820$. Auch das kleinere \texttt{DeepSeek-R1-Distill-Qwen-14B} überzeugt mit einem F1-Score von $0{,}848$ und einer Precision von $0{,}829$. Diese Modelle übertreffen die proprietären Benchmarks und erfüllen die Qualitätsziele klar.

Die europäischen Open-Source-Modelle weisen hingegen ein heterogenes Leistungsbild auf. \texttt{Mistral-Large-Instruct-2411} ist mit einem F1-Score von $0{,}823$ und einer Precision von $0{,}779$ das leistungsstärkste offene EU-Modell und erfüllt alle Qualitätsziele. \texttt{Mistral-7B-Instruct-v0.3} erreicht zwar einen Recall von $0{,}856$, bleibt mit einer Precision von $0{,}712$ und F1-Score von $0{,}777$ aber unter den Zielwerten. \texttt{Mixtral-8x7B-Instruct-v0.1} schneidet mit einem F1-Score von $0{,}596$ und Recall von $0{,}550$ deutlich schlechter ab und verfehlt alle Ziele.

Insgesamt dominieren bei den Open-Source-Modellen die chinesischen Qwen-\linebreak~Varianten und die GPT-OSS-Modelle das Feld, während europäische Modelle im Schnitt hinter den internationalen Spitzenreitern zurückbleiben.

\paragraph{UF4: Wie gut schneiden Open-Source-Modelle gegenüber kommerziellen Modellen wie GPT-4o ab?}

Der Vergleich zeigt, dass mehrere Open-Source-Modelle die kommerziellen Vertreter in der Klassifikationsqualität übertreffen. Zu den proprietären Modellen im Vergleichsfeld zählen \texttt{GPT-4o} und das europäische \texttt{Mistral Medium 3.1}. \texttt{Mistral Medium 3.1} erreicht einen F1-Score von $0{,}843$, einen Recall von $0{,}877$ und eine Precision von $0{,}811$ und liegt damit vor \texttt{GPT-4o}, das einen F1-Score von $0{,}822$, einen Recall von $0{,}762$ und eine sehr hohe Precision von $0{,}892$ erzielt.

Unter den offenen Modellen führen \texttt{Qwen3-235B-A22B-Thinking-2507} mit einem F1-Score von $0{,}874$, einem Recall von $0{,}932$ und einer Precision von $0{,}824$ sowie \texttt{GPT-OSS-20B} mit einem F1-Score von $0{,}866$, einem Recall von $0{,}918$ und einer Precision von $0{,}820$. Das kleinere \texttt{DeepSeek-R1-Distill-Qwen-14B} liegt beim F1-Score mit $0{,}848$ ebenfalls über \texttt{GPT-4o}, erreicht jedoch nicht dessen außergewöhnlich hohe Precision. Gleichzeitig ist die Spannweite innerhalb der Open-Source-Kategorie größer, wie das schwache Abschneiden von \texttt{Mixtral-8x7B-\linebreak~Instruct-v0.1}.

Insgesamt zeigen hochwertige Open-Source-Modelle die stärkste Balance aus hohem Recall und hohem F1-Score und erfüllen die Qualitätsziele klar, während\linebreak~\texttt{GPT-4o} durch exzellente Precision überzeugt, aber aufgrund des niedrigen Recalls häufiger kritische Aktivitäten übersieht. \texttt{Mistral Medium 3.1} belegt, dass ein kommerzielles \ac{EU}-Modell die Ziele ebenfalls erfüllt und im F1-Score und Recall vor \texttt{GPT-4o} liegt.

Auf Basis der durchgeführten Experimente, Analysen und Antworten auf die Unterfragen lässt sich die Hauptforschungsfrage beantworten im Folgenden beantworten.

\paragraph{FF1: Wie zuverlässig identifizieren LLM DSGVO-kritische Aktivitäten in BPMN-Prozessmodellen?}

Die Ergebnisse zeigen, dass moderne \acp{LLM} \ac{DSGVO}-\linebreak~kritische Aktivitäten mit hoher Zuverlässigkeit erkennen können. Insgesamt erfüllten neun von dreizehn Modellen den F1-Score-Zielwert von mindestens $0{,}80$, darunter auch mehrere kleinere Modelle. Die besten Modelle – \texttt{Qwen3-235B-A22B-\linebreak~Thinking-2507}, \texttt{GPT-OSS-20B}, \texttt{DeepSeek-R1-Distill-Qwen-14B} und \texttt{Mistral Medium 3.1} – erzielten F1-Scores zwischen $0{,}843$ und $0{,}874$ sowie Recall-Werte von $0{,}868$ bis $0{,}932$, überschritten damit deutlich die aus der Literatur abgeleiteten Referenzwerte und erfüllten die Qualitätsziele. Gleichzeitig gibt es Modelle wie \texttt{Mixtral-8x7B-Instruct-v0.1} und \texttt{Qwen2.5-7B-Instruct}, die mit F1-Scores unter $0{,}60$ beziehungsweise $0{,}73$ und Recall unter $0{,}70$ deutlich abfallen.

Die Robustheitsanalyse zeigt, dass die meisten Modelle eine sehr geringe Varianz zwischen verschiedenen Seeds aufweisen: die Standardabweichung des F1-Scores liegt oft unter $0{,}02$. Modelle wie \texttt{Gemma-3-12B-it}, \texttt{Mistral-Large-\linebreak~Instruct-2411} und \texttt{DeepSeek-R1-Distill-Qwen-14B} benötigten zudem keine Retries, um eine korrekte JSON-Antwort zu liefern, was ihre Zuverlässigkeit unterstreicht. Dagegen reagieren \texttt{Mistral Medium 3.1}, \texttt{Qwen2.5-7B-Instruct} und vor allem \texttt{Mixtral-8x7B-Instruct-v0.1} empfindlicher auf den Seed und erfordern bei Formatierung der Ausgabe teilweise zahlreiche Wiederholungen.

Die Fallstudien verdeutlichen typische Fehlerbilder. Im Testfall \enquote{Sales Warehouse} erkannte \texttt{Qwen3-235B-A22B-Thinking-2507} alle kritischen Aktivitäten, markierte aber zusätzlich das Versenden eines Produkts als kritisch, weil das Modell die Nutzung der Kundenadresse als potenzielle Datenverarbeitung interpretierte, während bei der Modellierung lediglich an einen logistischen Schritt gedacht war. Dieses \ac{FP} ist angesichts des angestrebten hohen Recalls akzeptabel. Im Szenario \enquote{Marketing-Kampagne} identifizierte \texttt{GPT-OSS-20B} die anonymisierte Auswertung von Klickraten fälschlich als kritisch, da die notwendigen Kontextinformationen im \ac{BPMN}-Modell fehlten. Umgekehrt zeigte der Testfall \enquote{Karten-App – Standort erfassen}, dass \texttt{Mistral-Large-Instruct-2411} trotz vorhandener Datenassoziationen die zweite Aktivität \enquote{Route berechnen} nicht als kritisch markierte. Dieses \ac{FN} widerspricht dem Hauptziel eines maximalen Recalls und weist auf Schwächen bei der Erkennung von Datenflüssen über mehrere Schritte hin.

Zusammenfassend lässt sich festhalten, dass \acp{LLM} \ac{DSGVO}-kritische Aktivitäten in \ac{BPMN}-Prozessmodellen zuverlässig identifizieren können, sofern hochwertige Modelle eingesetzt werden. Die Top-Modelle erfüllen die definierten Qualitätsziele mit Recall-Werten über $0{,}90$ und akzeptabler Precision, die Zahl der \ac{FP} pro Prozess bleibt moderat. Dennoch sind eine sorgfältige Modellwahl und eine nachgelagerte menschliche Prüfung ratsam, um die verbleibenden \acp{FP} und \ac{FN} zu adressieren und die Ergebnisse in den jeweiligen Anwendungskontext einzuordnen.