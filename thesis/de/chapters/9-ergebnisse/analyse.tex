\section{Analyse}\label{sec:analyse}

Im Folgenden werden die Ergebnisse nach Modellkategorien aufgeschlüsselt und analysiert. Zunächst werden die Leistungen der Open-Weight-Modelle mit denen der proprietären Modelle verglichen. Anschließend werden die Modelle nach ihrer Größe in kleine und große Modelle unterteilt und deren Leistungen gegenübergestellt. Abschließend wird die Leistung der Modelle mit europäischem Bezug mit der der internationalen Modelle verglichen.

\subsection*{Proprietäre vs. Open-Weight Modelle}

Die beiden proprietären Modelle \texttt{GPT-4o} und \texttt{Mistral Medium 3.1} zeigen im direkten Vergleich mittelmäßige Leistungen. \textttt{Mistral Medium 3.1} erreicht einen F1-Score von $0{,}843$ und übertrifft damit das Referenzmodell \texttt{GPT-4o} mit $0{,}822$ leicht. Der höhere Recall von \texttt{Mistral Medium 3.1} von $0{,}877$ deutet darauf hin, dass dieses Modell sensiblen Aktivitäten eher als kritisch klassifiziert, während \texttt{GPT-4o} durch eine sehr hohe Precision von $0{,}892$ bei gleichzeitig niedrigem Recall von $0{,}762$ eher konservativ klassifiziert.

Die Open-Weight-Modelle präsentieren ein heterogenes Bild, wobei mehrere Modelle die proprietären Vertreter deutlich übertreffen. So erzielt \texttt{Qwen3-235B-A22B-\linebreak~Thinking-2507} als bestes Modell einen F1-Score von $0{,}874$ bei gleichzeitig höchstem Recall von $0{,}932$ und einer Precision von $0{,}824$. Auch weitere kleinere offene Modelle wie \texttt{GPT-OSS-20B} mit einem F1-Score von $0{,}866$ und einem Recall von $0{,}918$ \texttt{DeepSeek-R1-Distill-Qwen-14B} mit einem F1-Score von $0{,}848$ liegen beim F1-Score vor den proprietären Modellen. Die hohe Precision von \texttt{DeepSeek-\linebreak~R1-Distill-Qwen-14B} von $0{,}829$ und der hohe Recall von \texttt{Gemma-3-27B-it} von $0{,}879$ zeigen außerdem, dass Open-Weight-Modelle sowohl bei Precision als auch bei Recall überzeugen können.

Allerdings zeigt sich bei den offenen Modellen eine starke Varianz. Das europäische \texttt{Mixtral-8x7B-Instruct-v0.1} weist mit einem F1-Score von $0{,}596$, einer Precision von $0{,}652$ und einem Recall von lediglich $0{,}550$ die bei weitem schwächsten Ergebnisse auf. Insgesamt lässt sich festhalten, dass mehrere offene Modelle die proprietären Lösungen klar übertreffen, wobei die Leistungsunterschiede innerhalb der Open‑Source‑Kategorie sehr groß sind. Im nächsten Abschnitt werden die Modelle nach ihrer Größe kategorisiert und verglichen, um mögliche Zusammenhänge zwischen Modellgröße und Klassifizierungsleistung zu untersuchen.

\subsection*{Kleine vs. große Modelle}

Die Gegenüberstellung kleiner Modelle mit $\leq 25B$ und großer Modelle mit $> 25B$ Parametern in Tabelle \ref{tab:small-vs-large} zeigt nahezu keinen Unterschied im durchschnittlichen F1-Score mit $0{,}805$ vs. $0{,}806$. Wird jedoch der durchschnittliche F1-Score der großen Modelle ohne das Ausreißermodell \texttt{Mixtral-8x7B-Instruct-v0.1} betrachtet, ergibt sich mit $0{,}836$ ein deutlich höherer Wert. Dies deutet darauf hin, dass größere Modelle tendenziell bessere Leistungen erbringen können, aber auch anfälliger für Leistungseinbußen sind, wenn sie nicht optimal auf die Aufgabe abgestimmt sind. Auch Precision und Accuracy sind vergleichbar mit ein wenig besseren Werten bei den großen Modellen. Beim Recall zeigen die kleinen Modelle mit $0{,}843$ sogar einen leicht höheren Wert als die großen Modelle mit $0{,}839$. Allerdings ist die Standardabweichung bei den großen Modellen mit $0{,}089$ deutlich höher als bei den kleinen Modellen mit $0{,}057$, was auf eine größere Varianz in der Leistung der großen Modelle hinweist. Diese Varianz wird ebenfalls vor allem durch das Ausreißermodell \texttt{Mixtral-8x7B-Instruct-v0.1} verursacht, das mit einem Recall von nur $0{,}550$ deutlich schlechter abschneidet als die anderen großen Modelle.

\begin{table}[htbp]
 \centering
 \caption{Kleine vs. große Modelle: Mittelwerte je Gruppe und bestes Modell.}
 \label{tab:small-vs-large}
 \begin{adjustbox}{width=\textwidth}
  \begin{threeparttable}[width=\textwidth]
   \begin{tabular}[width=\textwidth]{l r r}
    \toprule
    \textbf{Metrik} & \textbf{Klein} ($\leq 25B$) & \textbf{Groß} ($> 25B$) \\
    \midrule
    Anzahl Modelle\tnote{1}             & 5                         & 8 \\
    Ø F1-Score $\pm$ SD\tnote{2}      & 0.805 $\pm$ 0.057                     & 0.806 $\pm$ 0.089 \\
    Ø Precision $\pm$ SD    & 0.774 $\pm$ 0.050                     & 0.779 $\pm$ 0.085 \\
    Ø Recall $\pm$ SD       & 0.843 $\pm$ 0.086                     & 0.839 $\pm$ 0.128 \\
    Ø Accuracy $\pm$ SD     & 0.744 $\pm$ 0.067                     & 0.749 $\pm$ 0.099 \\
    Bester F1-Score & 0.866                     & 0.874 \\
    Bestes Modell (F1-Score)   & GPT-OSS-20B               & Qwen3-235B-A22B-Thinking-2507 \\
    Bester Precision & 0.829                     & 0.892 \\
    Bestes Modell (Precision) & DeepSeek-R1-Distill-Qwen-14B        & GPT-4o \\
    Bester Recall & 0.918                     & 0.932 \\
    Bestes Modell (Recall)      & GPT-OSS-20B      & Qwen3-235B-A22B-Thinking-2507 \\
    Beste Accuracy & 0.821                     & 0.830 \\
    Bestes Modell (Accuracy)     & GPT-OSS-20B               & Qwen3-235B-A22B-Thinking-2507 \\
    \bottomrule
   \end{tabular}
   \begin{tablenotes}
    \footnotesize
    \item[1] Einteilung nach gesamten Milliarden Parametern bei \ac{MoE}. Die Proprietären Modelle \texttt{GPT-4o} und \texttt{Mistral Medium 3.1} wurden trotz fehlender Parameterangabe als große Modelle eingeordnet.
    \item[2] Ohne \texttt{Mixtral-8x7B-Instruct-v0.1} beträgt der Durchschnitt der großen Modelle $\pm$ SD $0.836 \pm 0.029$.
   \end{tablenotes}
  \end{threeparttable}
 \end{adjustbox}
\end{table}

Das beste kleine Modell \texttt{GPT-OSS-20B} erreicht mit $0{,}866$ einen F1-Score, der nur geringfügig unter dem besten großen Modell \texttt{Qwen3-235B-A22B-Thinking-2507} mit $0{,}874$ liegt. Außerdem hat \texttt{GPT-OSS-20B} mit einem Recall von $0{,}918$ ebenfalls nur einen minimal kleineren Wert als \texttt{Qwen3-235B-A22B-Thinking-2507} mit $0{,}932$ Insgesamt zeigen die Ergebnisse, dass die Modellgröße allein kein ausschlaggebender Faktor für die Klassifizierungsleistung ist. Vielmehr spielen die Trainingsdaten, die Feinabstimmung und die Architektur eine entscheidende Rolle. Die richtigen kleinen Modelle können mit den großen Modellen durchaus mithalten, was insbesondere für den praktischen Einsatz relevant ist, da kleinere Modelle oft ressourcenschonender und kostengünstiger betrieben werden können.

\subsection*{Europäische versus internationale Modelle}

Die Betrachtung der Herkunft zeigt, dass die europäischen Mistral-Modelle unterschiedlich stark abschneiden. \texttt{Mistral Medium 3.1} erreicht als bestes \ac{EU}-Modell einen F1-Score von $0{,}843$ und einen Recall von $0{,}877$ und hat damit sogar besser abgeschnitten als das internationale Benchmark-Modell \texttt{GPT-4o}, das einen F1-Score von $0{,}822$ und einen Recall von $0{,}762$ erreichte. Das größte europäische Open-Weight-Modell \texttt{Mistral-Large-Instruct-2411} liegt mit einem F1-Score von $0{,}823$ und einer sehr geringen Standardabweichung ebenfalls im Mittelfeld. Deutlich schwächer fallen \texttt{Mistral-7B-Instruct-v0.3} mit einem F1-Score von $0{,}777$ und insbesondere \texttt{Mixtral-8x7B-Instruct-v0.1} mit einem F1-Score von $0{,}596$ aus. Unter Berücksichtigung der Größenklassen sind die europäischen Modelle überwiegend in der unteren Ranglistenhälfte und bestenfalls im Mittelfeld vertreten. Die große Streuung der Ergebnisse verdeutlicht, dass die \ac{EU}-Modelle noch nicht durchgängig zur Spitzengruppe aufschließen und teilweise erhebliches Optimierungspotenzial aufweisen.

Die internationalen Modelle – hierzu zählen insbesondere die chinesischen Qwen- und DeepSeek-Modelle sowie die US-amerikanischen GPT-Modelle und die\linebreak~Gemma-Modelle – liegen auf der Rangliste tendenziell weiter vorne als die europäischen Mistral Modelle. \texttt{Qwen3-235B-A22B-Thinking-2507} erzielt mit einem F1-Score von $0{,}874$ und einem Recall von $0{,}932$ die besten Werte im gesamten Vergleich. Auch das kleinere \texttt{DeepSeek-R1-Distill-Qwen-14B} mit einem F1-Score von $0{,}848$ und \texttt{GPT-OSS-20B} mit einem F1-Score von $0{,}866$ liefern Ergebnisse, die über den europäischen Modellen liegen. Lediglich \texttt{Qwen2.5-7B-instruct} mit einem F1-Score von $0{,}724$ schneidet schlechter ab, als das gleich große europäische \texttt{Mistral-7B-Instruct-v0.3} mit einem F1-Score von $0{,}777$. Die starke Performance dieser internationalen Open-Weight-Modelle legt nahe, dass sie umfangreiche und vielfältige Trainingsdaten sowie ausgereifte Modellarchitekturen nutzen.

Gleichzeitig haben die EU‑Modelle den Vorteil, dass sie von Anbietern stammen, die strenger den europäischen Datenschutzbestimmungen unterliegen und sich daher besser für datenschutzsensible Szenarien eignen. Modelle wie \texttt{Mistral\linebreak~Medium 3.1} zeigen, dass leistungsfähige europäische Alternativen existieren, doch im Vergleich zu internationalen Spitzenmodellen besteht weiterhin eine Lücke. Eine tabellarische Übersicht der aggregierten Kennzahlen für beide Gruppen findet sich in Tabelle~\ref{tab:eu-vs-international}.

\begin{table}[htbp]
 \centering
 \caption{Europäische vs. internationale Modelle: Mittelwerte je Gruppe und bestes Modell.}
 \label{tab:eu-vs-international}
 \begin{adjustbox}{width=\textwidth}
  \begin{threeparttable}[width=\textwidth]
   \begin{tabular}[width=\textwidth]{l r r}
    \toprule
    \textbf{Metrik} & \textbf{\ac{EU}-Modelle} & \textbf{Internationale Modelle} \\
    \midrule
    Anzahl Modelle              & 4                           & 9 \\
    Ø F1-Score $\pm$ SD         & 0{,}760 $\pm$ 0{,}098       & 0{,}826 $\pm$ 0{,}045 \\
    Ø Precision $\pm$ SD        & 0{,}738 $\pm$ 0{,}061       & 0{,}797 $\pm$ 0{,}056 \\
    Ø Recall $\pm$ SD           & 0{,}789 $\pm$ 0{,}138       & 0{,}864 $\pm$ 0{,}076 \\
    Ø Accuracy $\pm$ SD         & 0{,}694 $\pm$ 0{,}101       & 0{,}771 $\pm$ 0{,}057 \\
    Bester F1-Score             & 0{,}843                     & 0{,}874 \\
    Bestes Modell (F1-Score)    & Mistral Medium 3.1          & Qwen3-235B-A22B-Thinking-2507 \\
    Bester Precision            & 0{,}811                     & 0{,}892 \\
    Bestes Modell (Precision)   & Mistral Medium 3.1          & GPT-4o \\
    Bester Recall               & 0{,}877                     & 0{,}932 \\
    Bestes Modell (Recall)      & Mistral Medium 3.1          & Qwen3-235B-A22B-Thinking-2507 \\
    Beste Accuracy              & 0{,}794                     & 0{,}830 \\
    Bestes Modell (Accuracy)    & Mistral Medium 3.1          & Qwen3-235B-A22B-Thinking-2507 \\
    \bottomrule
   \end{tabular}
   \begin{tablenotes}
    \footnotesize
    \item Die \ac{EU}‑Modelle umfassen \texttt{Mistral‑7B‑Instruct‑v0.3}, \texttt{Mixtral‑8x7B‑Instruct‑v0.1}, \texttt{Mistral‑Large‑Instruct‑2411} und \texttt{Mistral Medium 3.1}. Die internationalen Modelle sind die übrigen in Kapitel \ref{sec:ueberblick} betrachteten Modelle.
   \end{tablenotes}
  \end{threeparttable}
 \end{adjustbox}
\end{table}

 Die vorangegangene Analyse hat die Leistungsunterschiede zwischen proprietären und offenen Modellen sowie zwischen europäischen und internationalen Modellen aufgezeigt. Besonders Modelle wie \texttt{Qwen3‑235B‑A22B‑Thinking‑2507} und \texttt{GPT‑OSS‑20B} haben hervorragend abgeschnitten, während andere Modelle klare Schwächen offenbaren. Auf Basis dieser Erkenntnisse werden im folgenden Kapitel die formulierten Forschungsfragen beantwortet. Dabei wird untersucht welches Modell sich insgesamt am besten für die Identifikation \ac{DSGVO}‑kritischer Aktivitäten eignet.