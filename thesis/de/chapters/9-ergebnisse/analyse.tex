\section{Analyse nach Modellkategorien}\label{sec:analyse-nach-modellkategorien}

Für ein besseres Verständnis der Leistungsunterschiede werden die Modelle im Folgenden nach verschiedenen Kriterien gruppiert und verglichen. Dabei wird jeweils diskutiert, wie sich die Gruppen in Bezug auf die Qualitätsziele unterscheiden und welche Trends sich beobachten lassen. Die Implikationen für Praxis und Forschungsfragen werden am Ende dieses Kapitels in Abschnitt~\ref{sec:antworten-auf-forschungsfragen} gebündelt beantwortet.

\subsection*{Proprietäre versus Open-Weight Modelle}

Die beiden proprietären Modelle \texttt{GPT-4o} und \texttt{Mistral Medium 3.1} erreichen F1-Scores von $0{,}822$ bzw.\ $0{,}843$. Trotz seiner exzellenten Precision von $0{,}892$ verfehlt \texttt{GPT-4o} aufgrund des niedrigen Recalls von $0{,}762$ das Mindestziel und übersieht damit relativ viele kritische Aktivitäten. \texttt{Mistral Medium 3.1} bietet mit einem Recall von $0{,}877$ eine bessere Balance und erfüllt alle Qualitätsziele.

Die offene Kategorie zeigt ein heterogenes Bild. Mehrere Modelle wie \texttt{Qwen3-235B-\linebreak~A22B-Thinking-2507} mit einem F1-Score $= 0{,}874$, \texttt{GPT-OSS-20B} mit F1-Score $= 0{,}866$ und \texttt{DeepSeek-R1-Distill-Qwen-14B} mit F1-Score $= 0{,}848$ übertreffen die proprietären Modelle. Sie erkennen kritische Aktivitäten sehr zuverlässig und klassifizieren nur wenige unkritische Aktivitäten fälschlich als kritisch. Gleichzeitig gibt es mit \texttt{Mixtral-8x7B-Instruct-v0.1}, das F1-Score $= 0{,}596$ erzielte, auch klare Ausreißer nach unten, die weder genug kritische Aktivitäten erkennen noch eine akzeptable Precision bieten.

Insgesamt zeigt sich, dass hochwertige offene Modelle ein besseres Verhältnis von Recall und Precision aufweisen und die Qualitätsziele häufig klar erfüllen. Für die Praxis bedeutet dies, dass offene Modelle eine attraktive Alternative zu proprietären Lösungen darstellen, jedoch ist die Auswahl des Modells entscheidend, da die Leistungsunterschiede innerhalb der offenen Kategorie erheblich sind.

\subsection*{Kleine versus Große Modelle}

Tabelle \ref{tab:small-vs-large} vergleicht die Mittelwerte der Metriken für kleine Modelle ($\leq 25$\,B Parameter) und große Modelle ($>25$\,B Parameter). Im Durchschnitt unterscheiden sich die Gruppen nur geringfügig. Die kleinen Modelle erreichen einen mittleren F1-Score von $0{,}805$ und die großen Modelle $0{,}806$, wobei die großen Modelle ohne den Ausreißer \texttt{Mixtral-8x7B-Instruct-v0.1} einen leicht höheren Durchschnitt von $0{,}836$ erreichen. Der beste F1-Score unter den kleinen Modellen stammt von \texttt{GPT-OSS-20B} mit $0{,}866$, bei den großen Modellen führt \texttt{Qwen3-235B-A22B-\linebreak~Thinking-2507} mit $0{,}874$. Bemerkenswert ist der leicht höhere durchschnittliche Recall der kleinen Modelle von $0{,}843$ gegenüber den großen mit $0{,}839$, wohingegen Precision und Accuracy annähernd identisch sind.

\begin{table}[htbp]
 \centering
 \caption{Kleine vs. große Modelle: Durchschnittswerte pro Gruppe und jeweils bestes Modell.}
 \label{tab:small-vs-large}
 \begin{adjustbox}{width=\textwidth}
  \begin{threeparttable}[width=\textwidth]
   \begin{tabular}[width=\textwidth]{l r r}
    \toprule
    \textbf{Metrik} & \textbf{Klein} ($\leq 25B$) & \textbf{Groß} ($> 25B$) \\
    \midrule
    Anzahl Modelle\tnote{1}             & 5                         & 8 \\
    Ø F1-Score $\pm$ SD\tnote{2}      & 0,805 $\pm$ 0,057                     & 0,806 $\pm$ 0,089 \\
    Ø Precision $\pm$ SD    & 0,774 $\pm$ 0,050                     & 0,779 $\pm$ 0,085 \\
    Ø Recall $\pm$ SD       & 0,843 $\pm$ 0,086                     & 0,839 $\pm$ 0,128 \\
    Ø Accuracy $\pm$ SD     & 0,744 $\pm$ 0,067                     & 0,749 $\pm$ 0,099 \\
    Bester F1-Score & 0,866                     & 0,874 \\
    Bestes Modell (F1-Score)   & GPT-OSS-20B               & Qwen3-235B-A22B-Thinking-2507 \\
    Bester Precision & 0,829                     & 0,892 \\
    Bestes Modell (Precision) & DeepSeek-R1-Distill-Qwen-14B        & GPT-4o \\
    Bester Recall & 0,918                     & 0,932 \\
    Bestes Modell (Recall)      & GPT-OSS-20B      & Qwen3-235B-A22B-Thinking-2507 \\
    Beste Accuracy & 0,821                     & 0,830 \\
    Bestes Modell (Accuracy)     & GPT-OSS-20B               & Qwen3-235B-A22B-Thinking-2507 \\
    \bottomrule
   \end{tabular}
   \begin{tablenotes}
    \footnotesize
    \item[1] Einteilung nach gesamten Milliarden Parametern bei \ac{MoE}. Die Proprietären Modelle \texttt{GPT-4o} und \texttt{Mistral Medium 3.1} wurden trotz fehlender Parameterangabe als große Modelle eingeordnet.
    \item[2] Ohne \texttt{Mixtral-8x7B-Instruct-v0.1} beträgt der Durchschnitt der großen Modelle $\pm$ SD $0.836 \pm 0.029$.
   \end{tablenotes}
  \end{threeparttable}
 \end{adjustbox}
\end{table}

Diese Ergebnisse bestätigen, dass die Modellgröße allein kein Garant für eine bessere Klassifikationsleistung ist. Kleinere Modelle wie \texttt{GPT-OSS-20B} liefern sehr starke Screening-Leistung bei geringeren Kosten und lassen sich leichter On-\linebreak~Premises\footnote{
On-Premises bezeichnet den Betrieb von IT-Systemen im eigenen Rechenzentrum statt in der Cloud.
} betreiben. In der Praxis sollte daher das Auswahlkriterium für ein Modell die \emph{Balance aus Recall, Precision und Betriebskosten} sein, nicht die Parameterzahl.

\subsection*{Europäische vs. internationale Modelle}

Tabelle \ref{tab:eu-vs-international} stellt die Mittelwerte der europäischen Mistral-Modelle den übrigen internationalen Modellen gegenüber. Die europäischen Modelle zeigen eine größere Streuung: das kommerzielle \texttt{Mistral Medium 3.1} erfüllt mit einem F1-Score von $0{,}843$ und Recall von $0{,}877$ alle Zielkriterien und liegt knapp vor dem Referenzmodell \texttt{GPT-4o}. Ähnlich sieht es bei dem Open-Weight-Modell \texttt{Mistral-Large-\linebreak~Instruct-2411} aus. Dagegen verfehlen \texttt{Mistral-7B-Instruct-v0.3} und insbesondere \texttt{Mixtral-8x7B-Instruct-v0.1} die Qualitätsziele deutlich. Im Durchschnitt bleiben die europäischen Modelle hinter den internationalen Spitzenreitern zurück. Letztere – allen voran \texttt{Qwen3-235B-A22B-Thinking-2507} mit einem F1-Score von $0{,}874$ und \texttt{GPT-OSS-20B} mit $0{,}866$ – erreichen im Mittel einen höheren F1-Score sowie Recall und weisen eine geringere Varianz auf.

\begin{table}[htbp]
 \centering
 \caption{Europäische vs. internationale Modelle: Durchschnittswerte pro Gruppe und jeweils bestes Modell.}
 \label{tab:eu-vs-international}
 \begin{adjustbox}{width=\textwidth}
  \begin{threeparttable}[width=\textwidth]
   \begin{tabular}[width=\textwidth]{l r r}
    \toprule
    \textbf{Metrik} & \textbf{\ac{EU}-Modelle} & \textbf{Internationale Modelle} \\
    \midrule
    Anzahl Modelle              & 4                           & 9 \\
    Ø F1-Score $\pm$ SD         & 0{,}760 $\pm$ 0{,}098       & 0{,}826 $\pm$ 0{,}045 \\
    Ø Precision $\pm$ SD        & 0{,}738 $\pm$ 0{,}061       & 0{,}797 $\pm$ 0{,}056 \\
    Ø Recall $\pm$ SD           & 0{,}789 $\pm$ 0{,}138       & 0{,}864 $\pm$ 0{,}076 \\
    Ø Accuracy $\pm$ SD         & 0{,}694 $\pm$ 0{,}101       & 0{,}771 $\pm$ 0{,}057 \\
    Bester F1-Score             & 0{,}843                     & 0{,}874 \\
    Bestes Modell (F1-Score)    & Mistral Medium 3.1          & Qwen3-235B-A22B-Thinking-2507 \\
    Bester Precision            & 0{,}811                     & 0{,}892 \\
    Bestes Modell (Precision)   & Mistral Medium 3.1          & GPT-4o \\
    Bester Recall               & 0{,}877                     & 0{,}932 \\
    Bestes Modell (Recall)      & Mistral Medium 3.1          & Qwen3-235B-A22B-Thinking-2507 \\
    Beste Accuracy              & 0{,}794                     & 0{,}830 \\
    Bestes Modell (Accuracy)    & Mistral Medium 3.1          & Qwen3-235B-A22B-Thinking-2507 \\
    \bottomrule
   \end{tabular}
   \begin{tablenotes}
    \footnotesize
    \item Die \ac{EU}‑Modelle umfassen \texttt{Mistral‑7B‑Instruct‑v0.3}, \texttt{Mixtral‑8x7B‑Instruct‑v0.1}, \texttt{Mistral‑Large‑Instruct‑2411} und \texttt{Mistral Medium 3.1}. Die internationalen Modelle sind die übrigen in Kapitel \ref{sec:ueberblick} betrachteten Modelle.
   \end{tablenotes}
  \end{threeparttable}
 \end{adjustbox}
\end{table}

Diese Vergleiche belegen, dass ein europäischer Ursprung nicht zwangsläufig mit einer geringeren Leistung einhergeht – \texttt{Mistral Medium 3.1} erreicht gute Werte, dicht gefolgt von \texttt{Mistral-Large-Instruct-2411}. Allerdings zeigen die Ergebnisse auch, dass einige europäische Modelle hinter den internationalen Konkurrenten zurückbleiben. Insgesamt sind die internationalen Modelle im Durchschnitt leistungsfähiger und stabiler, da sie die europäischen Modelle in jeder Metrik im Durchschnitt übertreffen und eine geringere Varianz aufweisen. Zudem ist in jeder Metrik ein internationales Modell führend.