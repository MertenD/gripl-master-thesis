\section{Modellvorstellung}\label{sec:modellvorstellung}

Tabelle \ref{tab:models-all} stellt die für diese Arbeit ausgewählten Modelle kurz vor. Der Stichtag der Modellauswahl war der 30.\,September\,2025. Im Folgenden wird die Modellauswahl im Detail erläutert.

\begin{sidewaystable}[htbp]
    \centering
    \caption{Übersicht a
    ller Modelle mit technischen Eckdaten (Stand 30.09.2025).}
    \label{tab:models-all}
    \begin{threeparttable}
        \begin{subtable}[t]{0.49\linewidth}
            \centering
            \subcaption{Technische Eckdaten und Herkunft der Modelle.}
            \begin{tabular}{@{}p{6.3cm} r r r}
                \toprule
                \textbf{Modell} & \textbf{Parameter (B)} & \textbf{Kontext (Tokens)} & \textbf{Herkunftsland} \\
                \midrule
                Mistral-7B-Instruct-v0.3 & 7.24 & 32{.}000  & Frankreich \cite{HF_Mistral7B_2025} \\
                Mixtral-8x7B-Instruct-v0.1 & 46.7 total / 12.9 aktiv\tablefootnote{Mixtral nutzt \ac{MoE} mit 8 Experten als Architekur. Die Gesamtparameterzahl bezieht sich auf alle Experten, die aktive Parameterzahl auf den jeweils genutzten Expertenanteil pro Inferenzdurchlauf \cite{Mixtral_Blog}.} & 32{.}000 & Frankreich \cite{HF_Mixtral8x7B_2025, Mixtral_Blog} \\
                Mistral-Large-Instruct-2411 & 123 & 128{.}000 & Frankreich \cite{HF_MistralLargeInstruct_2025} \\
                Mistral-Medium-3.1 & n.v. & 128{.}000 & Frankreich \cite{mistral_models_overview} \\
                Gemma-3-12B-it & 12.2 & 128{.}000 & Großbritannien\tablefootnote{Google DeepMind hat seinen Hauptsitz in London, gehört jedoch zu Alphabet (USA). Wo genau trainiert wurde, ist unklar.\\\\\\\\} \cite{HF_Gemma3_12B_2025} \\
                Gemma-3-27B-it & 27.4 & 128{.}000 & Großbritannien \cite{HF_Gemma3_27B_2025} \\
                Qwen2.5-7B-Instruct & 7.62 & 131{.}072 & China \cite{HF_Qwen7B_2025} \\
                Qwen3-235B-A22B-Thinking-2507 & 235 & 256{.}000 & China \cite{HF_Qwen3_235B_2025} \\
                DeepSeek-R1-Distill-Qwen-14B & 14.8 & 131{.}072 & China \cite{HF_DeepSeekR1_Distill_Qwen14B_2025} \\
                DeepSeek-V3.1 & 671 total / 37 aktiv & 128{.}000 & China \cite{HF_DeepSeek_V3_1_2025} \\
                GPT-OSS-20B & 20.91 total / 3.61 aktiv & 131{.}072 & USA \cite{OpenAI_GPTOSS_ModelCard_2025} \\
                GPT-OSS-120B & 116.83 total / 5.13 aktiv & 131{.}072 & USA \cite{OpenAI_GPTOSS_ModelCard_2025} \\
                GPT-4o (2024-11-20) & n.v. & 128{.}000 & USA \cite{openai-hello-gpt-4o} \\
                \bottomrule
            \end{tabular}
        \end{subtable}
        \hfill
        \begin{subtable}[t]{0.49\linewidth}
            \centering
            \subcaption{Lizenz, letzte Updates und Downloads der Modelle.}
            \begin{tabular}{@{}p{6.3cm} l r r@{}}
                \toprule
                \textbf{Modell} & \textbf{Lizenz} & \textbf{Letztes Update} & \textbf{Downloads} \\
                \midrule
                Mistral-7B-Instruct-v0.3 & Apache-2.0 & 24.07.2025 & 687{.}000 \cite{HF_Mistral7B_2025} \\
                Mixtral-8x7B-Instruct-v0.1 & Apache-2.0 & 24.07.2025 & 43{.}500 \cite{HF_Mixtral8x7B_2025} \\
                Mistral-Large-Instruct-2411 & Mistral Research License & 28.07.2025 & 4{.}200 \cite{HF_MistralLargeInstruct_2025, MRL_Research_License} \\
                Mistral-Medium-3.1 & Proprietär & 25.08.2025 & n.v. \cite{mistral_models_overview} \\
                Gemma-3-12B-it & Gemma & 21.03.2025 & 523{.}000 \cite{Gemma3_License, HF_Gemma3_12B_2025} \\
                Gemma-3-27B-it & Gemma & 21.03.2025 & 1{.}180{.}000 \cite{Gemma3_License, HF_Gemma3_27B_2025} \\
                Qwen2.5-7B-Instruct & Apache-2.0 & 12.01.2025 & 5{.}100{.}000 \cite{HF_Qwen7B_2025} \\
                Qwen3-235B-A22B-Thinking-2507 & Apache-2.0 & 17.08.2025 & 52{.}900 \cite{HF_Qwen3_235B_2025} \\
                DeepSeek-R1-Distill-Qwen-14B & MIT & 24.02.2025 & 341{.}000 \cite{HF_DeepSeekR1_Distill_Qwen14B_2025} \\
                DeepSeek-V3.1 & MIT & 05.09.2025 & 447{.}000 \cite{HF_DeepSeek_V3_1_2025} \\
                GPT-OSS-20B & Apache-2.0 & 26.08.2025 & 6{.}450{.}000 \cite{OpenAI_GPTOSS_ModelCard_2025} \\
                GPT-OSS-120B & Apache-2.0 & 26.08.2025 & 3{.}600{.}000 \cite{OpenAI_GPTOSS_ModelCard_2025} \\
                GPT-4o (2024-11-20) & Proprietär & 20.11.2024 & n.v. \cite{openai-hello-gpt-4o} \\
                \bottomrule
            \end{tabular}
        \end{subtable}
    \end{threeparttable}
\end{sidewaystable}

Zusätzlich zu Herkunft, Lizenz, Größe und Kontext wurden die Modelle anhand \emph{etablierter Benchmarks für generelle Fähigkeiten} validiert: (1) \emph{MMLU-Pro} \cite{MMLU-Pro} - eine robustere Weiterentwicklung von MMLU \cite{MMLU} - das fachübergreifendes Wissen und anspruchsvolles Schlussfolgern mit zehn Antwortoptionen pro Aufgabe prüft. (2) \emph{BIG-bench Hard (BBH)} \cite{BBH}, das mehrschrittiges Denken auf 23 besonders schwierigen Aufgaben evaluiert. (3) \emph{Commonsense}-Benchmarks wie \emph{HellaSwag} \cite{hellaswag} und \emph{WinoGrande} \cite{winogrande}, die alltagstaugliches Verständnis prüfen. Bei HellaSwag wählt das Modell die plausibelste Fortsetzung einer kurzen Szene und bei WinoGrande klärt es, worauf sich ein Pronomen im Satz bezieht. (4) \emph{AIME} \cite{MAA_AIME_2025}, ein anspruchsvoller Mathematik-Benchmark auf Grundlage der \emph{American Invitational Mathematics Examination}, bewertet präzises mehrschrittiges Problemlösen an wettbewerbsnahen Aufgaben. In diesen Benchmarks schneiden die in Tabelle~\ref{tab:models-all} aufgeführten Modelle in ihren jeweiligen Größenklassen durchweg gut ab. Dies wird u.\,a.\ durch die veröffentlichten Auswertungen von Mistral \cite{Mistral_Large_Bench, mistral_models_benchmarks}, OpenAI \cite{OpenAI_GPTOSS_ModelCard_2025}, Google DeepMind \cite{HF_Gemma3_12B_2025, HF_Gemma3_27B_2025}, Alibaba \cite{HF_Qwen3_235B_2025, Qwen2p5_TechReport_2025} sowie DeepSeek \cite{HF_DeepSeekR1_Distill_Qwen14B_2025,HF_DeepSeek_V3_1_2025} gestützt.

Die französische Firma Mistral AI bietet mehrere leistungsstarke Modelle an, die zum großteil offene Gewichte haben. Durch ihre Herkunft repräsentieren die Mistral Modelle in dieser Arbeit die \ac{EU}-Modelle. \textbf{Mistral-7B-Instruct-v0.3} \cite{HF_Mistral7B_2025} ist ein 7,24\,B Parameter großes Modell mit einem Kontextfenster von 32{.}000 Tokens. Es wurde speziell für Anweisungsfolgen (engl. Instruct) optimiert und ist unter der Apache-2.0-Lizenz frei verfügbar. Das \textbf{Mixtral-8x7B-Instruct-v0.1} Modell \cite{HF_Mixtral8x7B_2025} nutzt eine Mixture-of-Experts-Architektur mit insgesamt 46,7\,B Parametern, von denen jedoch nur 12,9\,B aktiv genutzt werden. Es hat ebenfalls ein Kontextfenster von 32{.}000 Tokens und ist unter der Apache-2.0-Lizenz verfügbar. Das \textbf{Mistral-Large-Instruct-2411} Modell \cite{HF_MistralLargeInstruct_2025} ist mit 123\,B Parametern deutlich größer und bietet ein Kontextfenster von 128{.}000 Tokens. Es wird unter der Mistral Research License veröffentlicht, die die Nutzung auf nicht-kommerzielle Forschung beschränkt \cite{MRL_Research_License}. Das Modell \textbf{Mistral-Medium-3.1} \cite{mistral_models_overview} bietet ein Kontextfenster von 128{.}000 Tokens und gilt als aktuelles Spitzenmodell der Mistral-Modellreihe. Anders als die übrigen Mistral-Modelle ist es proprietär und wird von Mistral~AI auf \ac{EU}-Servern unter Beachtung der \ac{DSGVO} betrieben \cite{mistral-gdpr, mistral-data-storing}. Damit ist die Verarbeitung sensibler Daten möglich. Das Modell eignet sich daher - trotz nicht veröffentlichter Gewichte - für den Einsatz in datenschutzkritischen Szenarien.

Die Gemma-3-Modelle von Google Deepmind repräsentieren eine neue Generation multimodaler \acp{LLM} mit offenen Gewichten. Die hier betrachteten Varianten sind \textbf{Gemma-3-12B-it} \cite{HF_Gemma3_12B_2025} mit 12,2\,B Parametern und \textbf{Gemma-3-27B-it} \cite{HF_Gemma3_27B_2025} mit 27,4\,B Parametern. Beide Modelle verfügen über ein großes Kontextfenster von 128{.}000 Tokens und sind unter der proprietären Gemma-Lizenz veröffentlicht, die eine breite kommerzielle Nutzung erlaubt \cite{Gemma3_License}. Die genaue Herkunft der Modelle ist unklar, da Google DeepMind seinen Hauptsitz in Großbritannien hat, jedoch zu Alphabet in den USA gehört. Wo genau die Modelle trainiert wurden, ist nicht bekannt.

Die Qwen-Modelle wurden von Alibaba Cloud in China entwickelt. Das kleinere Modell \textbf{Qwen2.5-7B-Instruct} \cite{HF_Qwen7B_2025} hat 7,62\,B Parameter, ein Kontextfenster von 131{.}072 Tokens und ist unter der Apache-2.0-Lizenz frei verfügbar. Das größere Modell \textbf{Qwen3-235B-A22B-Thinking-2507} \cite{HF_Qwen3_235B_2025} verfügt über 235\,B Parameter, ein Kontextfenster von 256{.}000 Tokens und ist ebenfalls unter der Apache-2.0-Lizenz veröffentlicht. Zugleich ist bei außerhalb der \ac{EU} entwickelten Modellen - daher auch bei den chinesischen Qwen-Modellen - besondere Vorsicht geboten, da die Trainingsdaten und -methoden nicht immer transparent sind und möglicherweise nicht den europäischen Datenschutzstandards und -ethiken entsprechen.

Das chinesische Unternehmen DeepSeek AI hat mit \textbf{DeepSeek-R1-Distill-Qwen-14B} \cite{HF_DeepSeekR1_Distill_Qwen14B_2025} ein Modell mit 14{,}8\,B Parametern veröffentlicht, das auf Qwen-2.5 basiert, ein Kontextfenster von 131{.}072 Tokens bietet und unter der MIT-Lizenz frei verfügbar ist. Das größere Modell \textbf{DeepSeek-V3.1} \cite{HF_DeepSeek_V3_1_2025} setzt auf eine Mixture-of-Experts-Architektur mit insgesamt 671\,B Parametern, von denen pro Token 37\,B aktiv sind. Es bietet ein Kontextfenster von 128{.}000 Tokens und ist ebenfalls MIT-lizenziert. Besonders bemerkenswert sind die DeepSeek-Modelle, weil DeepSeek im Januar 2025 mit \emph{DeepSeek-R1-Zero} \cite{HF_DeepSeekR1_Zero_2025} eines der ersten permissiv lizenzierten Reasoning-Modelle in OpenAI-Größenordnung vorlegte und zugleich einen Trainings-Ansatz etablierte, bei dem \ac{LLM}-Reasoning nahezu ausschließlich über Reinforcement Learning erlernt wird \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}.

\textbf{GPT-4o} \cite{openai-hello-gpt-4o} ist ein proprietäres Modell von OpenAI mit einem Kontextfenster von 128{.}000 Tokens. Es wurde am 20.\,November\,2024 veröffentlicht und ist das einzige internationale proprietäre Modell in dieser Arbeit. Die genauen Parameterzahlen sind nicht bekannt. GPT-4o wird über die OpenAI-API bereitgestellt und ist das erste Omni-Modell von OpenAI, das neben Text auch Bilder als Eingabe akzeptieren kann. Durch ChatGPT \cite{chatgpt} sind die Funktionen und Fähigkeiten des Modells bekannt und es gilt als bestes Modell für \enquote{generelle Fähigkeiten}. Dadurch gilt es als der De-facto-Standard in der Industrie. Das Modell dient in diesem Vergleich als Referenzpunkt für den aktuellen Stand der Technik. Mit den GPT-OSS Modellen \cite{OpenAI_GPTOSS_ModelCard_2025} hat OpenAI zudem zwei Modelle mit offenen Gewichten unter Apache-2.0-Lizenz veröffentlicht, die explizit für Forschung und kommerzielle Nutzung freigegeben sind. Das \textbf{GPT-OSS-20B} Modell hat 20,91\,B Parameter (3,61\,B aktiv) und ein Kontextfenster von 131{.}072 Tokens. Das größere \textbf{GPT-OSS-120B} Modell verfügt über 116,83\,B Parameter (5,13\,B aktiv) und das gleiche Kontextfenster. Sie sind spannende Vergleichsmodelle, da sie von einem der führenden \ac{LLM}-Anbieter stammen und dennoch offen verfügbar sind.

Insgesamt deckt die Modellauswahl in dieser Arbeit eine breite Palette von Modellgrößen, Architekturen und Lizenztypen ab. Die Mistral-Modelle repräsentieren die \ac{EU}-Modelle mit offenen Gewichten, während die Gemma-, Qwen- und GPT-OSS-Modelle internationale Alternativen aus verschiedenen Herkunftsländern darstellen. Die DeepSeek-Modelle bieten innovative Ansätze im Reasoning-Bereich, und GPT-4o dient als aktueller Industriestandard. Diese Vielfalt ermöglicht eine umfassende Evaluation der Modelle hinsichtlich ihrer Eignung für die Klassifizierungsaufgabe von \ac{BPMN}-Modellen. Im nächsten Kapitel wird aufbauend auf den vorgestellten Modellen der Versuchsaufbau und die Durchführung der Experimente beschrieben.